---
title: 传统机器学习算法-语雀
date: 2020-04-01
categories: AI
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

介绍传统机器学习算法相关知识点，具体详见[《传统机器学习课程》](https://www.yuque.com/docs/share/3c5ecd1f-64a0-4fd1-9c85-8ce88dea0bce?#)

<!-- more -->

### 第1章 玩转机器学习
欢迎大家来到《Python3玩转机器学习》的课堂。在这个课程中，我们将从0开始，一点一点进入机器学习的世界。本门课程对机器学习领域的学习，绝不不仅仅只是对算法的学习，还包括诸如算法的评价，方法的选择，模型的优化，参数的调整，数据的整理，等等一系列工作。准备好了吗？现在开始我们的机器学习之旅！
- 1-1 什么是机器学习
- 1-2 课程涵盖的内容和理念
- 1-3 课程所使用的主要技术栈

**机器学习算法工程师需要具备哪些技能：**
- 数据敏感性，观察力
- 数学抽象能力，数学建模能力和数学工具的熟练使用的能力
- 能随手编脚本代码的能力，强大的计算机算法编程能力，高级开发工程师的素质
- 想象力，耐性和信心，较强的语言表达能力，抗打击能力

### 第2章 机器学习基础
机器学习到底是什么鬼？这一章将带领大家深入理解机器学习的世界，让大家去熟悉那些看似陌生的专业术语。监督学习，非监督学习，半监督学习，增强学习，批量学习，在线学习，参数学习，非参数学习。看完这一章，这些概念你就统统了解啦。不仅如此，本章还包含相当深刻地和机器学习相关的哲学探讨，让你深入思索有关机器学习
- 2-1 机器学习世界的数据
- 2-2 机器学习的主要任务
- 2-3 监督学习，非监督学习，半监督学习和增强学习
- 2-4 批量学习，在线学习，参数学习和非参数学习
- 2-5 和机器学习相关的“哲学”思考
- 2-6 课程使用环境搭建

`机器学习`本质是一项监督分类/回归问题，"监督"表示你有许多样本，假设你知道这些样本的正确答案，我们不断地把样本交给机器，并告诉机器这些样本是正确或错误的，对机器进行训练，最终达到机器学习的目的。

**监督学习，非监督学习，半监督学习、增强学习：**
1）监督学习： 有train set，train set里面y的取值已知。
2）无监督学习：有train set, train set里面y的取值未知。
3）半监督学习：有train set， train set里面y的取值有些知道有些不知道。
 4）增强学习：reinforcement learning， 无train set。

监督学习，通常对具有标记分类的训练样本特征进行学习，标记即已经知道其对应正确分类答案；而学习则本质是找到特征与标签（正确答案）之间的关系（函数），从而当训练结束，输入无标签的数据时，可以利用已经找出的关系方法进行分析得出数据标签。

反之，无监督学习则通常学习数据只有特征向量，没有标签（答案），学习模型通过学习特征向量发现其内部规律与性质，从而把数据分组聚类（Clustering）。无监督学习更类似我们的真实世界，去探索发现一些规律及分类。

**批量学习，在线学习，参数学习、非参数学习：**
大部分算法是集中处理所有的数据，也就是一口气对整个数据集进行建模与学习，并得到最佳假设。这种策略被称为`批量学习`（batch learning）。和批量学习相对应的是`在线学习`（online learning）。在在线学习中，数据是以细水长流的方式一点点使用，算法也会根据数据的不断馈入而动态地更新。当存储和计算力不足以完成大规模的批量学习时，在线学习不失为一种现实的策略。

在学校中，老师可以通过将学生代入学习过程，引导学生主动提问来加强学习效果。这种策略应用在机器学习中就是`主动学习`（active learning）。主动学习是策略导向的学习策略，通过有选择地询问无标签数据的标签来实现迭代式的学习。当数据的标签的获取难度较高时，这种方法尤其适用。

大多数情况下，机器学习的任务是求解输入输出单独或者共同符合的概率分布，或者拟合输入输出之间的数量关系。从数据的角度看，如果待求解的概率分布或者数量关系可以用一组有限且固定数目的参数完全刻画，求出的模型就是`参数模型`（parametric model）；反过来，不满足这个条件的模型就是`非参数模型`（non-parametric model）。当对所要学习的问题知之甚少的时候，不懂装懂地搞些先验分布往数据上生搬硬套就不是合理的选择，最好的办法反而是避免对潜在模型做出过多的假设。这类不使用先验信息，完全依赖数据进行学习得到的模型就是非参数模型。

1）最小二乘回归
`最小二乘回归`是常见的线性回归方法。最小二乘法的基本原则是：最优拟合直线应该使各点到直线的距离的和最小，也可表述为距离的平方和最小。
2）岭回归 
`岭回归 `是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。
岭回归是对最小二乘回归的一种补充，它损失了无偏性，来换取高的数值稳定性，从而得到较高的计算精度。通常岭回归方程的R平方值会稍低于普通回归分析，但回归系数的显著性往往明显高于普通回归，在存在共线性问题和病态数据偏多的研究中有较大的实用价值。
3）LASSO回归
`LASSO回归`其实本质是一种降维方法， 由Tibshirani(1996)提出。这种算法通过构造一个惩罚函数获得一个精炼的模型；通过最终确定一些指标的系数为零，LASSO算法实现了指标集合精简的目的。这是一种处理具有复共线性数据的有偏估计。Lasso的基本思想是在回归系数的绝对值之和小于一个常数的约束条件下，使残差平方和最小化，从而能够产生某些严格等于0的回归系数，得到解释力较强的模型。
4）LARS回归
`LARS回归`（英文名： Least Angle Regression，最小角回归）。Efron于2004年提出的一种变量选择的方法，类似于向前逐步回归(Forward Stepwise)的形式。从解的过程上来看它是LASSO回归的一种高效解法。
5）支持向量回归 
`SVR支持向量回归`（英文名： Support-Vector Regression）。支持向量机（ SVM ）是一种比较好的实现了结构风险最小化思想的方法。它的机器学习策略是结构风险最小化原则 为了最小化期望风险，应同时最小化经验风险和置信范围）
6）CART回归 
`CART`（英文名： Classification And Regression Tree分类回归树），是一种很重要的机器学习算法，既可以用于创建分类树（Classification Tree），也可以用于创建回归树（Regression Tree）。 将CART用于回归分析时就叫做CART回归。
CART算法的重要基础包含以下三个方面：
- 二分(Binary Split)：在每次判断过程中，都是对观察变量进行二分。CART算法采用一种二分递归分割的技术，算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝。因此CART算法生成的决策树是结构简洁的二叉树。因此CART算法适用于样本特征的取值为是或非的场景，对于连续特征的处理则与C4.5算法相似。
- 单变量分割(Split Based on One Variable)：每次最优划分都是针对单个变量。
- 剪枝策略：CART算法的关键点，也是整个Tree-Based算法的关键步骤。剪枝过程特别重要，所以在最优决策树生成过程中占有重要地位。有研究表明，剪枝过程的重要性要比树生成过程更为重要，对于不同的划分标准生成的最大树(Maximum Tree)，在剪枝之后都能够保留最重要的属性划分，差别不大。反而是剪枝方法对于最优树的生成更为关键。

7）CART分类
将CART用于分类问题时需要构建CART分类树。 创建分类树递归过程中，CART每次都选择当前数据集中具有最小Gini信息增益的特征作为结点划分决策树。ID3算法和C4.5算法虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支、规模较大，CART算法的二分法可以简化决策树的规模，提高生成决策树的效率。对于连续特征，CART也是采取和C4.5同样的方法处理。为了避免过拟合(Overfitting)，CART决策树需要剪枝。预测过程当然也就十分简单，根据产生的决策树模型，延伸匹配特征值到最后的叶子节点即得到预测的类别。


### 第3章 Jupyter,numpy,matplotlib
工欲善其事，必先利其器。在本章，我们将学习和机器学习相关的基础工具的使用：Jupyter Notebook, numpy和matplotlib。大多数教程在讲解机器学习的时候，大量使用这些工具，却不对这些工具进行系统讲解。我特意添加了这个章节，让同学们在后续编写机器学习算法的过程中，更加得心应手！...
- 3-1 Jupyter Notebook基础
- 3-2 Jupyter Notebook中的魔法命令
- 3-3 Numpy数据基础
- 3-4 创建Numpy数组(和矩阵)
- 3-5 Numpy数组(和矩阵)的基本操作
- 3-6 Numpy数组(和矩阵)的合并与分割
- 3-7 Numpy中的矩阵运算
- 3-8 Numpy中的聚合运算
- 3-9 Numpy中的arg运算
- 3-10 Numpy中的比较和Fancy Indexing
- 3-11 Matplotlib数据可视化基础
- 3-12 数据加载和简单的数据探索

**Jupyter**
如果你想使用Python学习数据分析或数据挖掘，那么它应该是你第一个应该知道并会使用的工具，它很容易上手，用起来非常方便，是个对新手非常友好的工具。而事实也证明它的确很好用，在数据挖掘平台 Kaggle 上，使用 Python 的数据爱好者绝大多数使用 jupyter notebook 来实现分析和建模的过程，因此，如果你想学习机器学习，数据挖掘，那么这款软件你真的应该了解一下。

`Jupyter notebook` 是一种 Web 应用，它能让用户将说明文本、数学方程、代码和可视化内容全部组合到一个易于共享的文档中，非常方便研究和教学。在原始的 Python shell 与 IPython 中，可视化在单独的窗口中进行，而文字资料以及各种函数和类脚本包含在独立的文档中。但是，notebook 能将这一切集中到一处，让用户一目了然。
Jupyter notebook特别适合做数据处理，其用途可以包括数据清理和探索、可视化、机器学习和大数据分析。
1. 安装Jupyter notebook
在 conda 环境下安装 Jupyter notebook 可以使用 `conda install jupyter notebook`。当然，也可以通过 pip 来安装 `pip install jupyter notebook`。
2. 启动 notebook 服务器
在终端环境下输入 `jupyter notebook`， 服务器就会在当前操作的目录下启动。启动后，默认的 notebook 服务器的运行地址是 http://localhost:8888。可以通过点击 “New” 创建新的 notebook、文本文件、文件夹或终端。
3. 执行程序
notebook 中的大部分工作均在代码单元格中完成。编写和执行代码都在这里，就像我们平时在 IDE 软件里敲代码一样，给变量赋值、定义函数和类、导入包等。执行单元格代码可以通过 `Shift + Enter `来完成。

**numpy**
详见[《菜鸟教程之numpy》](https://www.runoob.com/numpy)
`Numpy数据基础`：bool_、int_、int8、int16、int32、int64、float_、float16、float32、complex_、complex64……
`基本属性`：ndim（维度数值）、shape（数组形状）、size（元素个数）、dtype（数据类型）、itemsize（每个元素所占子节）、data（数组实际元素的缓存区）
`创建Numpy数组(和矩阵)`：
- numpy.array 创建数组 a = np.array([2,3,4])
- numpy.empty(shape, dtype = float, order = 'C') 创建空数组
- numpy.zeros(shape, dtype = float, order = 'C') 创建全0数组
- numpy.ones(shape, dtype = None, order = 'C') 创建全1数组
- numpy.asarray(a, dtype = None, order = None) 从已有的数组创建数组 x =  [1,2,3]  a = np.asarray(x)  
- numpy.frombuffer(buffer, dtype = float, count = -1, offset = 0)
- numpy.fromiter(iterable, dtype, count=-1)
- numpy.arange(start, stop, step, dtype) 从数值范围创建数组
- numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None) 创建等差数列
- np.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None) 创建等比数列

`Numpy数组(和矩阵)的基本操作`：
1. 修改数组形状
- reshape——numpy.reshape(arr, newshape, order='C')不改变数据的条件下修改形状
- flat——numpy.ndarray.flat 是一个数组元素迭代器
- flatten——ndarray.flatten(order='C')返回一份数组拷贝，对拷贝所做的修改不会影响原始数组
- ravel	返回展开数组，numpy.ravel() 展平的数组元素，顺序通常是"C风格"，返回的是数组视图
2. 翻转数组
- transpose	对换数组的维度
- ndarray.T	和 self.transpose() 相同
- rollaxis	向后滚动指定的轴
- swapaxes	对换数组的两个轴
3. 修改数组维度
- broadcast	产生模仿广播的对象
- broadcast_to	将数组广播到新形状
- expand_dims	扩展数组的形状
- squeeze	从数组的形状中删除一维条目
4. 连接数组
- concatenate	连接沿现有轴的数组序列
- stack	沿着新的轴加入一系列数组。
- hstack	水平堆叠序列中的数组（列方向）
- vstack	竖直堆叠序列中的数组（行方向）
5. 分割数组
- split	将一个数组分割为多个子数组
- hsplit	将一个数组水平分割为多个子数组（按列）
- vsplit	将一个数组垂直分割为多个子数组（按行）
6. 数组元素的添加与删除
- resize	返回指定形状的新数组
- append	将值添加到数组末尾
- insert	沿指定轴将值插入到指定下标之前
- delete	删掉某个轴的子数组，并返回删除后的新数组
- unique	查找数组内的唯一元素

`Numpy中的矩阵运算`：
- dot	两个数组的点积，即元素对应相乘。
- vdot	两个向量的点积
- inner	两个数组的内积
- matmul	两个数组的矩阵积
- determinant	数组的行列式
- solve	求解线性矩阵方程
- inv	计算矩阵的乘法逆矩阵
`Numpy中的聚合运算`：
`Numpy中的arg运算`：
`Numpy中的比较和Fancy Indexing`：

**matplotlib**
[matplotlib中文手册](https://www.matplotlib.org.cn/)
[matplotlib绘图入门详解](https://www.jianshu.com/p/da385a35f68d)

### 第4章 k近邻算法
kNNk近邻算法本身是一个思想非常简单的算法，但是这个简单算法背后，也蕴含着丰富的内容。在这一章，我们将详细介绍k近邻算法的原理，进而对训练数据集，测试数据集，分类准确度，超参数，数据归一化，样本距离等基础概念进行详细的探讨。我们将详细了解scikit-learn框架中对算法的封装，并实现我们自己的算法框架。我们还将学...
- 4-1 k近邻算法基础
- 4-2 scikit-learn中的机器学习算法封装
- 4-3 训练数据集，测试数据集
- 4-4 分类准确度
- 4-5 超参数
- 4-6 网格搜索与k近邻算法中更多超参数
- 4-7 数据归一化
- 4-8 scikit-learn中的Scaler
- 4-9 更多有关k近邻算法的思考

kNN算法的核心思想是用距离最近(多种衡量距离的方式)的k个样本数据来代表目标数据的分类。
具体讲，存在训练样本集， 每个样本都包含数据特征和所属分类值。
输入新的数据，将该数据和训练样本集汇中每一个样本比较，找到距离最近的k个，在k个数据中，出现次数做多的那个分类，即可作为新数据的分类。

**其算法的描述为：**
1）计算测试数据与各个训练数据之间的距离；
2）按照距离的递增关系进行排序；
3）选取距离最小的K个点；
4）确定前K个点所在类别的出现频率；
5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。

[使用python机器学习五 （scikit-learn基础）](https://www.jianshu.com/p/180b6d93c5a4)
[使用python机器学习六（scikit-learn实战）](https://juejin.im/entry/596b0f1af265da6c4977ab12)

**scikit-learn 主要提供了以下功能：**
- 测试数据集，sklearn.datasets模块提供了乳腺癌、kddcup 99、iris、加州房价等诸多开源的数据集
- 降维(Dimensionality Reduction): 为了特征筛选、统计可视化来减少属性的数量。
- 特征提取(Feature extraction)： 定义文件或者图片中的属性。
- 特征筛选(Feature selection)： 为了建立监督学习模型而识别出有真实关系的属性。
- 按算法功能分类，分为监督学习：分类（classification）和回归（regression），以及非监督学习：聚类（clustering）。sklearn提供了很全面的算法实现。
- 聚类(Clustring): 使用KMeans之类的算法，给未标记的数据分类。
- 交叉验证(Cross Validation): 评估监督学习模型的性能。
- 参数调优(Parameter Tuning): 调整监督学习模型的参数以获得最大效果。
- 流型计算(Manifold Learning): 统计和描绘多维度的数据

使用scikit-learn实现KNN：
```python
from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
# fit a k-nearest neighbor model to the data
model = KNeighborsClassifier()
model.fit(X, y)
print(model)
# make predictions
expected = y
predicted = model.predict(X)
# summarize the fit of the model
print(metrics.classification_report(expected, predicted))
print(metrics.confusion_matrix(expected, predicted))
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')
           
             precision    recall  f1-score   support
        0.0       0.83      0.88      0.85       500
        1.0       0.75      0.65      0.70       268
avg / total       0.80      0.80      0.80       768
[[442  58]
 [ 93 175]]
```
### 第5章 线性回归法
线性回归法是机器学习领域的经典算法，很多更复杂的算法都是以线性回归为基础的。在这一章，我们将深入学习线性回归法背后的原理，同时仔细探讨如何评价回归算法。大家将对MSE,RMSE,MAE和R Squared等回归问题的评价指标有充分的理解。在实现层面上，我们还将学习机器学习领域的一个重要的实现技巧：向量化。
- 5-1 简单线性回归
- 5-2 最小二乘法
- 5-3 简单线性回归的实现
- 5-4 向量化
- 5-5 衡量线性回归法的指标：MSE，RMSE和MAE
- 5-6 最好的衡量线性回归法的指标：R Squared
- 5-7 多元线性回归和正规方程解
- 5-8 实现多元线性回归
- 5-9 使用scikit-learn解决回归问题
- 5-10 线性回归的可解释性和更多思考

**线性回归**（Linear Regression），数理统计中回归分析，用来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，其表达形式为y = w'x+e，e为误差服从均值为0的正态分布，其中只有一个自变量的情况称为简单回归，多个自变量的情况叫多元回归。

**最小二乘法**：代价函数中使用的均方误差，其实对应了我们常用的欧几里得的距离（欧式距离，Euclidean Distance）, 基于均方误差最小化进行模型求解的方法称为“最小二乘法”（least square method），即通过最小化误差的平方和寻找数据的最佳函数匹配。微积分角度来讲，最小二乘法是采用非迭代法，针对代价函数求导数而得出全局极值，进而对所给定参数进行估算。概率论角度来讲，如果数据的观测误差是/或者满足高斯分布，则最小二乘解就是使得观测数据出现概率最大的解，即最大似然估计-Maximum Likelihood Estimate，MLE（利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值）。

**MSE，RMSE和MAE：**
1. MSE: Mean Squared Error
均方误差是指参数估计值与参数真值之差平方的期望值;
MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。
2. RMSE
均方误差:均方根误差是均方误差的算术平方根
3. MAE :Mean Absolute Error
平均绝对误差是绝对误差的平均值
平均绝对误差能更好地反映预测值误差的实际情况.

**R Squared：**
在一个线性回归问题中，我们使用 R 平方（R-Squared）来判断拟合度。根据  R 平方（R-Squared）的取值，来判断模型的好坏：如果结果是 0，说明模型拟合效果很差；如果结果是 1，说明模型无错误。`一般来说，R-Squared 越大，表示模型拟合效果越好。`R-Squared 反映的是大概有多准，因为，随着样本数量的增加，R-Square必然增加，无法真正定量说明准确程度，只能大概定量。它的计算方法为

<img width="40%" src="https://i.loli.net/2020/04/01/sTze2MZtbYjEK9C.jpg" alt="R平方" />

### 第6章 梯度下降法
梯度下降法是在机器学习领域的一个重要的搜索策略。在这一章，我们将详细讲解梯度下降法的基本原理，一步一步改进梯度下降算法，让大家理解梯度下降法中各种参数，尤其是学习率的意义。同时，我们还将引申出随机梯度下降法和小批量梯度下降法两个方法，让大家对梯度下降法家族有一个全方位的认识。...
- 6-1 什么是梯度下降法
- 6-2 模拟实现梯度下降法
- 6-3 线性回归中的梯度下降法
- 6-4 实现线性回归中的梯度下降法
- 6-5 梯度下降法的向量化和数据标准化
- 6-6 随机梯度下降法
- 6-7 scikit-learn中的随机梯度下降法
- 6-8 如何确定梯度计算的准确性？调试梯度下降法
- 6-9 有关梯度下降法的更多深入讨论

`梯度下降`是优化中最流行的算法之一，也是目前用于优化神经网络最常用到的方法。同时，每个优秀的深度学习库都包含了优化梯度下降的多种算法的实现（比如，lasagne、caffe 和 keras 的文档）。然而，这些算法一般被封装成优化器，如黑盒一般，因此很难得到它们实际能力和缺点的解释。
1. 批量梯度下降BGD
因为我们需要计算完整个数据集的梯度才能更新，批量梯度下降非常的耗时，而且面对无法完全放入内容的数据集，处理起来也很棘手。批量梯度更新也无法让我们在线，即在运行时加入新的样本进行模型更新。
2. 随机梯度下降SGD
对于大型的数据集，批量梯度下降会出现冗余计算的情况，因为在每次参数更新前，对于相似的样本，它会重新计算梯度。SGD 可以单次更新，所以可以避免这个问题。因此，它常常速度更快，而且可以用于在线学习。
3. Mini-batch 梯度下降
Mini-batch 最终吸取了两种方法的优点，每次使用 mini-batch 的训练数据进行更新。这样，它既可以减少参数更新的方差，让收敛更加的稳定；又可以利用常见最新的深度学习库中高度优化的矩阵进行优化，使更加 mini-batch 的梯度计算更加高效。一般 mini-batch 的大小在 50 和 256 之间，但可以根据不同的应用具体选择。mini-batch 梯度下降一般是训练神经网络的算法选择，而在使用 mini-batch 时也会选择使用 SGD。
4. 动量
本质上讲，在使用动量时，就像我们往山下推下一个球。球在下落的过程中累积动量，会在路上变得越来越快（直到在空气阻力的作用下达到速度的极值，即 $\gamma < 1$）。对于参数的更新亦是如此：动量在梯度指向同一方向上时会增加，而在梯度变化方向时会减小。这样，我们就可以更快收敛，并可以减小震荡。
5. Nesterov 梯度加速
然而，就像丢下山的球，盲目地沿着斜坡下降，这还是无法让人满意。我们希望可以有个更加智能的球，可以有它要去哪里的概念，这样再次上坡时就知道要减速了。Nesterov 梯度加速就是可以给动量带来这种感知能力的方法。我们知道我们将会使用动量 $\gamma v_{t-1}$ 来移动参数 $\theta$。这样计算 $ \theta - \gamma v_{t-1} $ 可以得到参数在下个位置的近似值（对于完全更新来说是缺少梯度的），可以大略知道参数将会是什么。现在我们可以高效地不是依据当前梯度，而是依据参数的未来近似位置来计算梯度。
6. Adagrad
Adagrad3 是这样的基于梯度优化算法：根据参数自适应地更新学习率，对于不频繁更新的参数做较大更新，而对于频繁变化的参数做较小更新。因此，这个算法非常适合处理稀疏数据。
7. RMSprop
RMSprop 也是通过一个指数延迟梯度平方的平均值来除以学习率。Hinton 建议 $\gamma$ 设置为 0.9，而学习率 $\eta$ 的较好的默认值为0.001。
8. Adam
自适应矩估计（Adaptive Moment Estimation，Adam）[15]是另一个计算各个参数的自适应学习率的方法。除了像 Adadelta 和 RMSprop 一样存储过去梯度平方 $v_t$ 的指数移动平均值外，Adam 还保留了一个类似动量的过去梯度 $m_t$ 的指数移动平均值：
$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$
$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$

### 第7章 PCA与梯度上升法
通常教材会使用非常多的数学概念来讲解PCA，在这个课程中，我们将另辟蹊径，绕开繁重的数学概念，使用梯度下降法的姊妹方法：梯度上升法来求解PCA问题，进而深刻理解PCA的基本原理，如何使用PCA进行数据的降维。我们还将给出多个PCA的应用场景，不仅让大家亲手实践出PCA降维的巨大威力，也让大家看到PCA在降噪，人脸识别等...
- 7-1 什么是PCA
- 7-2 使用梯度上升法求解PCA问题
- 7-3 求数据的主成分PCA
- 7-4 求数据的前n个主成分
- 7-5 高维数据映射为低维数据
- 7-6 scikit-learn中的PCA
- 7-7 试手MNIST数据集
- 7-8 使用PCA对数据进行降噪
- 7-9 人脸识别与特征脸

PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。

### 第8章 多项式回归与模型泛化
在这一章，我们将接触非线性问题。我们将学习多项式回归的思想，使用线性回归的思路来解决非线性问题。进一步，我们将引申出或许是机器学习领域最重要的一个问题：模型泛化问题。我们将深入探讨什么是欠拟合，什么是过拟合，怎样检测欠拟合和过拟合。什么是交叉验证，什么是模型正则化。听起来拗口的Ridge和Lasso都是什么鬼...
- 8-1 什么是多项式回归
- 8-2 scikit-learn中的多项式回归与Pipeline
- 8-3 过拟合与欠拟合
- 8-4 为什么要有训练数据集与测试数据集
- 8-5 学习曲线
- 8-6 验证数据集与交叉验证
- 8-7 偏差方差平衡
- 8-8 模型泛化与岭回归
- 8-9 LASSO
- 8-10 L1, L2和弹性网络

### 第9章 逻辑回归
据统计，逻辑回归是机器学习领域最常用的分类算法，没有之一！在这一章，我们将逐渐揭开逻辑回归的神秘面纱，了解如何应用线性回归的思路，来解决分类问题。我们将综合之前所学习的很多内容，一点一点来完善我们的逻辑回归模型。我们还将继续深入分类问题，学习对分类结果概率的估计，以及决策边界等重要概念。 ...
- 9-1 什么是逻辑回归
- 9-2 逻辑回归的损失函数
- 9-3 逻辑回归损失函数的梯度
- 9-4 实现逻辑回归算法
- 9-5 决策边界
- 9-6 在逻辑回归中使用多项式特征
- 9-7 scikit-learn中的逻辑回归
- 9-8 OvR与OvO

### 第10章 评价分类结果
对机器学习分类算法结果的评估，是一个公认的复杂问题。在这一章，我们将来阐述这个问题为什么复杂。我们如何更好地看待我们的机器学习算法给出的结果。学习诸如混淆矩阵，准确率，精确率，召回率，F1，以及ROC等诸多评价分类结果的指标。通过这一章的学习，大家将更好地理解自己的机器学习算法给出的结果，从而在实际应用...
- 10-1 准确度的陷阱和混淆矩阵
- 10-2 精准率和召回率
- 10-3 实现混淆矩阵，精准率和召回率
- 10-4 F1 Score
- 10-5 精准率和召回率的平衡
- 10-6 精准率-召回率曲线
- 10-7 ROC曲线
- 10-8 多分类问题中的混淆矩阵

### 第11章 支撑向量机 SVM
在这一章，我们将深入学习大名鼎鼎的支撑向量机SVM。我们将从线性SVM开始，理解SVM的思路，进而深入理解SVM解决非线性问题的方式——核函数。我们将重点学习两个最重要的核函数：多项式核和径向基函数核。我们更会使用真实的数据集实验，看到SVM的优缺点。最后，我们还将探讨使用SVM解决回归问题的思路。 ...
- 11-1 什么是SVM
- 11-2 SVM背后的最优化问题
- 11-3 Soft Margin SVM
- 11-4 scikit-learn中的SVM
- 11-5 SVM中使用多项式特征和核函数
- 11-6 到底什么是核函数
- 11-7 RBF核函数
- 11-8 RBF核函数中的gamma
- 11-9 SVM思想解决回归问题

### 第12章 决策树
在这一章，我们将学习另外一个大名鼎鼎的机器学习算法：决策树。决策树本身非常简单，背后并没有复杂的数学模型，但使用好决策树也有很多技巧。我们将深入了解什么是熵模型，什么是基尼系数，怎样使用决策树解决分类问题，怎样获得分类的概率，怎样用决策树解决回归问题，以及使用决策树的注意事项。 ...
- 12-1 什么是决策树
- 12-2 信息熵
- 12-3 使用信息熵寻找最优划分
- 12-4 基尼系数
- 12-5 CART与决策树中的超参数
- 12-6 决策树解决回归问题
- 12-7 决策树的局限性

### 第13章 集成学习和随机森林
集成学习的思想是机器学习领域解决问题的一种重要思想。我们将从集成之前已经学习过的算法出发，进而引入集成学习的经典算法：随机森林。我们将看到集成学习的威力。在这一章，我们还会对其他集成学习的思想，如AdaBoost, Gradient Boosting, Stacking等算法进行介绍。 ...
- 13-1 什么是集成学习
- 13-2 Soft Voting Classifier
- 13-3 Bagging 和 Pasting
- 13-4 oob (Out-of-Bag) 和关于Bagging的更多讨论
- 13-5 随机森林和 Extra-Trees
- 13-6 Ada Boosting 和 Gradient Boosting
- 13-7 Stacking

### 第14章 更多机器学习算法
相信通过这个课程的学习。同学们学到的不仅仅是一个一个零散的机器学习算法，更对机器学领域解决问题的方式有了一个系统性的认识。学会了这种思维方法，相信大家都可以更好地继续深入学习机器学习。在最后，我将给大家介绍scikit-learn的文档，希望大家能够借助scikit-learn这个强大的机器学习库，继续探索机器学习这个当下...
- 14-1 scikit-learn文档
- 14-2 继续深入机器学习的学习