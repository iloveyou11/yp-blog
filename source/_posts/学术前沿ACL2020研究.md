---
title: 学术前沿ACL2020研究
date: 2020-07-12
categories: AI
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

#### 基于Knowledge Embedding的多跳知识图谱问答
论文：[Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings](https://arxiv.org/abs/1910.03262v1)
多跳知识图谱问答指的是，该问答系统需要通过知识图谱上的多条边执行推理，以获得正确答案。

#### 万能的BERT——文本纠错
原生的 BERT 在一些NLP任务如error detection、NER中表现欠佳，说明预训练阶段的学习目标中对相关模式的捕获非常有限，需要根据任务进行一定改造。为了提高错误检测能力，本文在SOTA方法的基础上又添加了一个错误检测网络。分错误检测和纠正两步走。先检测每一个字的错误概率，然后根据检测结果将可能的错别字 soft-mask,再输给基于Bert的修正网络。这样就强制修正网络学习了错别字的上下文。

[论文链接](https://arxiv.org/pdf/2005.07421.pdf)

#### 突破瓶颈，打造更强大的Transformer(*) 
`推荐阅读`
针对Multi-Head Attention中可能存在建模瓶颈，提出了不同的方案来改进Multi-Head Attention
1. 再小也不能小key_size
如果固定一个比较大的key_size（比如128），那么我们可以调整模型的hidden_size和head数，使得参数量可以跟原始的BERT设计一致，但是效果更优！
2. 再缺也不能缺Talking
当前的Multi-Head Attention每个head的运算是相互孤立的，而通过将它们联系（Talking）起来，则可以得到更强的Attention设计

#### Deformer：让笨重的BERT问答匹配模型变快
[《Deformer：Decomposing Pre-trained Transformers for Faster Question Answering》](https://awk.ai/assets/deformer.pdf)
这篇文章提主要提出了一种变形的计算方式DeFormer，使问题和文档编码在低层独立编码再在高层交互，从而使得可以离线计算文档编码来加速QA推理和节省内存。

#### GPT-3诞生，Finetune也不再必要了
[《Language Models are Few-Shot Learners》](https://arxiv.org/abs/2005.14165)
GPT-3主要聚焦于更通用的NLP模型，解决当前BERT类模型的两个缺点：
- 对领域内有标签数据的过分依赖：虽然有了预训练+精调的两段式框架，但还是少不了一定量的领域标注数据，否则很难取得不错的效果，而标注数据的成本又是很高的。
- 对于领域数据分布的过拟合：在精调阶段，因为领域数据有限，模型只能拟合训练数据分布，如果数据较少的话就可能造成过拟合，致使模型的泛化能力下降，更加无法应用到其他领域。

#### 变矮又能变瘦的DynaBERT了解一下(*) 
`推荐阅读`
[《DynaBERT: Dynamic BERT with Adaptive Width and Depth》]()
提出了新的训练算法，同时对不同尺寸的子网络进行训练，通过该方法训练后可以在推理阶段直接对模型裁剪。依靠新的训练算法，本文在效果上超越了众多压缩模型，比如DistillBERT、TinyBERT以及LayerDrop后的模型。

说到模型压缩，常用的方法有以下几种：
1. 量化
2. 低轶近似／权重共享
3. 剪枝
4. 蒸馏

论文对于BERT的压缩流程是这样的：
- 训练时，对宽度和深度进行裁剪，训练不同的子网络
- 推理时，根据速度需要直接裁剪，用裁剪后的子网络进行预测

#### FastBERT：放飞BERT的推理速度(*) 
`推荐阅读`
《FastBERT: a Self-distilling BERT with Adaptive Inference Time》
提出了一种新的inference速度提升方式，相比单纯的student蒸馏有更高的确定性，且可以自行权衡效果与速度，简单实用。