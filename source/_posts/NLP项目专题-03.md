---
title: NLP项目专题-03
date: 2020-07-28
categories: AI
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

[NLP项目专题-01-文本分类](https://iloveyou11.github.io/2020/07/26/NLP%E9%A1%B9%E7%9B%AE%E4%B8%93%E9%A2%98-01/)
[NLP项目专题-02-自动补全](https://iloveyou11.github.io/2020/07/27/NLP%E9%A1%B9%E7%9B%AE%E4%B8%93%E9%A2%98-02/)
[NLP项目专题-03-语义消歧](https://iloveyou11.github.io/2020/07/28/NLP%E9%A1%B9%E7%9B%AE%E4%B8%93%E9%A2%98-03/)

### 语义消歧方法

消歧任务主要分以下几种：
1. 分词的消歧
2. 多义词的具体词义
3. 词性的判断

对于词性的判断，可以看做一个词性标注的问题，通常需要考虑邻近上下文。如果是词义判决，可能会有相隔很远的词语来决定其词性。因此大部分的词性标注模型简单地使用当前上下文，而语义消歧模型通常使用规模广泛一些的上下文中的实词。

```
消歧方法分为以下几种：
(1)有监督消歧
(2)无监督消歧
(3)基于词典的消歧
```

#### 有监督消歧
有监督消歧是采用已经消歧的语料库用来训练，在这个样本训练集中，歧义词`w`每次出现都被标注上一个语义标签，为有监督消歧提供了一个统计分类的实例，统计分类任务就是构建一个分类器，根据上下文`c`对新的歧义词进行分类。目前有两个已经用到语义消歧中的算法：一是贝叶斯分类，二是基于信息论的方法。贝叶斯分类是把上下文看成是一个无结构词集，整合了上下文窗口中众多的词汇信息。基于信息论方法仅仅考虑了上下文中的一个信息特征，此信息特征可以很灵敏地反映上下文结构。

##### （1）贝叶斯分类的原理

考虑一个上下文窗口中歧义词周围词的信息。通常我们这里使用一个特殊的分类器，即朴素贝叶斯分类器，这里使用朴素贝叶斯有两个假设：
- 第一个是上下文中的所有结构和词语顺序都可以被忽略。
- 可有重复的单词集中出现的词独立于其他词。

虽然这两个假设不太成立，但是朴素贝叶斯能在这个任务中取得一定的效果。s=argmaxp(Sk|c),Sk是W可能包含的语义，C是歧义词的上下文，而s是使该概率最大的语义，即消歧后确定的语义。

这里补充一下贝叶斯公式的原理（小插曲），它同样可以用来进行垃圾邮件分类和单词拼写纠错，以下是垃圾邮件分类的实例：
```
贝叶斯公式为 P(A|B)=P(B|A)*P(A)/P(B)
贝叶斯公式可以理解为 后验概率 = (似然度 * 先验概率)/标准化常量 ，即后验概率与先验概率和似然度的乘积成正比。

使用“贝叶斯方法”过滤垃圾邮件，还具有自我学习能力，会根据新收到的邮件，不断调整。收到的垃圾邮件越多，它的准确率就越高。
假定出现“发票”一词，此邮件定为垃圾邮件的概率如下：
```

<img src="https://i.loli.net/2020/07/28/o2Rfa6VNhkiOgH5.png" alt="贝叶斯-垃圾邮件分类" width="60%" />

```
我们继续计算，如果
P（正常）= P（垃圾）=50%
P(发票|垃圾)=5%
P(发票|正常)=0.1%
则 P(垃圾|发票)=(5%×50%) / (5%×50% + 0.1%×50%)=98%
因此，这封新邮件是垃圾邮件的概率是98%。

但是，我们仍然不能判断，这封邮件有98%的概率是垃圾邮件。因为：
（1）P(发票|垃圾) 和 P(发票|正常)是我们假定的，怎样实际计算它们？
（2）正常邮件也是可能含有“发票”这个词，误判了怎么办？

此时，我们需要建立历史资料库，对过滤器进行"训练"。
解析所有邮件，提取每一个词。然后，计算每个词语在正常邮件和垃圾邮件中的出现频率。
有了这个初步的统计结果，概率值计算问题就解决了。过滤器就可以投入使用了。
```

以下是贝叶斯用于单词拼写纠错的实例：
```
用户输入词汇w，这个词汇w可能是是一个错误单词，系统需要猜测用户真正想要输入的词汇是什么，记猜测词汇为c。

根据问题就是求 P(c|w) 的最大值，根据贝叶斯公式：P(c|w) = P(w|c)P(c) / P(w) ，因为对于不同的具体猜测c1,c2,c3...，P(w)都是一样的，所在在比较P(c1|w)和P(c2|w)时可以忽略P(w)这个参数，所以P(c|w) = P(w|c)P(c) / P(w) ∝ P(w|c)P(c)  ，这样就等价于求P(w|c)P(c)最大值。

（1）P(c)是在词库中出现 c 的概率，为先验概率
（2）P(w|c), 在用户输入词库中单词 c 的情况下敲成 w 的概率，即猜测生成我们观测数据的可能性的大小。

这时，就需要使用到“编辑距离”的概念，编辑距离0表示不作变换，编辑距离n变换表示对单词做n次字母的增删改查。
我们假设P(w|c, edit_0) >> P(w|c, edit_1) >> P(w|c, edit_2)
如果edit_1_set为空再计算edit2情形，依次类推，一般的应用中，只计算到edit2即可。
```

我们再回到有监督消歧算法，根据`P(A|B)=P(B|A)*P(A)/P(B)`贝叶斯公式，假设歧义词为`w`，P(w)为歧义词`w`出现的概率，`c`表示上下文，`P(w|c)`表示上下文c中出现w的概率，`P(c|w)`表示w出现时上下文为c的概率。因此我们使用到公式`P(c|w)=P(w|c)*P(c)/P(w)`来计算单词的意思，即`P(c|w)∝P(w|c)P(c)`

##### （2）基于信息论的方法

以W包含2个语义为例，基本思想是最大化互信息I(P,Q)，P是W的语义集，Q是W的指示器取值集（指示器即能区分W不同语义的关键邻近词）。例：法语“prendre”的含义是take或make，其指示器可以是decision,note,example,measure。P划分为p1={take，}和p2={make，}，Q分为Q1={note,example,measure,}和Q2={decision},如果W的指示器为note，出现在Q1中，那么W对应的语义应该对应地出现在P1中，即take。在这里，P和Q的集合划分的原则是最大化I(P,Q)。

#### 无监督消歧
无监督消歧并不是没有一点人工的干预，无监督依然需要部分监督数据来完成。无监督消歧主要是使用EM算法对W的上下文C进行无监督的聚类，也就是对W的语义进行了分类。（当然，该分类的结果不见得就是和词典中对该词的定义分类是匹配的）。

EM算法是按照最大似然原理，先随便指定一个分布参数，根据分布归类到某一部分，根据归类重新统计数目，按照最大似然估计分布参数，再重新归类、调参、估计，最终得出最优解。

无监督词义消歧的方法包括以下几种：
```
1. 基于知识的无监督
  1）基于机读词典的词义消歧
  2）基于义类词典的词义消歧
  3）基于结构化语义关系的图论式词义消歧
2. 基于统计的无监督
  1）基于聚类的词义消歧
  2）基于双语语料的词义消歧
  3）基于web的词义消歧
```

#### 基于词典的消歧
基于词典的消歧本质上也是无监督消歧的一种
`基于语义定义的消歧`
如果词典中对W的第i种定义包含词汇Ei，那么如果在一个包含W的句子中，同时也出现了Ei，那么就认为在该句子中W的语义应该取词典中的第i种定义。
`基于类义辞典的消歧`
词的每个语义都定义其对应的主题或范畴（如“网球”对应的主题是“运动”），多个语义即对应了多个主题。如果W的上下文C中的词汇包含多个主题，则取其频率最高的主题，作为W的主题，确定了W的主题后，也就能确定其对应的语义。
`基于双语对比的消歧`
这种方法比较有创意，即把一种语言作为另一种语言的定义。例如，为了确定“interest”在英文句子A中的含义，可以利用句子A的中文表达，因为interest的不同语义在中文的表达是不同的。如果句子A对应中文包含“存款利率”，那么“interest”在句子A的语义就是“利率”。如果句子A的对应中文是“我对英语没有兴趣”，