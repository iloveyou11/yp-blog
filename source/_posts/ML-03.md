---
title: ML系列3：深度学习中的问题解答
date: 2019-11-29
categories: ML
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

以下是机器学习中涉及到的常见问题的解答：

<!-- more -->

**为什么正则化项是阻止过拟合的好方法？**
regularizer提供了一个叫λ的旋钮，调大能让模型不要太随性（欠拟合），调小能让模型不要太呆板（过拟合）。一般来说，L1正则化项能够减少神经元的个数（不激活），L2正则项可以降低神经元的权重值w，从而降低过拟合的风险。

`L1范数（L1 norm）`是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。
比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|.
简单总结一下就是：
`L1范数:` 为x向量各个元素绝对值之和。
`L2范数: `为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或Frobenius范数
`Lp范数: `为x向量各个元素绝对值p次方和的1/p次方.
在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。
L1范数可以使权值稀疏，方便特征提取。
L2范数可以防止过拟合，提升模型的泛化能力。

**什么是PCA主成分分析？**
PCA是对高维数据进行降维，在模型训练中可以提升训练的速度。
通过投影将高维数据投影到1维、2维、3维或n维空间上，在设定保留数据结构信息（比如90%）的条件下，通过trial and error筛选出一个合适的K值，实现最大化降低维度和最大化保留数据信息的博弈。
通过比较不同线（平面、空间）之间的投影距离，选取投影距离之和最短的线作为K=1情况下的最优线。最后K歌eigenvectors组合成一个matrix，matrix的transpositon对原数据做变形，变形后的结果就是降维后的新数据。

**PCA的主成分维度如何选择？**
如何决定降维到哪个维度（1维、2维、3维或n维）K？
将K=1,2,3...分别带入到一个不等式中，选择哪个最能满足不等式成立的K值即可。
不等式：设定好我们希望PCA保留原有数据variance信息的百分比（如90%），不等式核心内容是做最大限度降维与最大化保留数据结构信息的博弈。

**为什么PCA不适合避免过拟合？**
过拟合是因为训练的模型过度地拟合了原有数据的分布，造成了测试集上的巨大误差。所以没有Y值，就谈不上overfitting。但是PCA只关注特征X，不关注Y，降维后仍保持X极高比例的variance信息，但没有任何依据说新生成的低维数据能提炼任何与Y值相关的信息。

**反欺诈中所用到的机器学习模型有哪些？**
反欺诈最简单的模型的`anomaly detection`。
本质：属于非监督学习，通过计算每个X对应的概率值，低于某阈值就归异常，高于某阈值就为正常。
算法：用训练数据X，获取X与正常状态的概率函数图，面对每个新X，计算每个维度上的概率，最终结果取决于所有维度概率之积。

**机器视觉的主要任务和难点挑战是什么？**
主要任务：
- 图像分类，即对图像做单一物体识别
- 目标检测，即对图像做多物体识别，同时用方框框住物体
- 风格转移，内容不变，风格被另一张图像的风格代替
- 图像检索，类似以图搜图任务
- 语义分割
- 无人驾驶
- 人脸识别
- 姿势识别
- ……

面临的挑战：
- 图片像素多，数据特征多，导致模型的参数多
- 如果数据量不能大幅超过参数数量，容易过拟合
- 占用大量内存，计算效率低下
- 计算量惊人，训练效率低下

特点：
- 每个隐藏神经元都在提炼学习特征，越深层filter提炼越抽象复杂的特征
- CNN重视空间信息的保留，眼里只有局部小特征

**DFF和CNN的区别：**
input：
- DFF input打破了空间信息，压缩成一个vector，作为整体被学习背后的特征
- CNN input保留了空间信息，截取成多个局部小图片，分别被学习背后数据特征

output：
- DFF的weights完成工作后，生成单一scalar，是对全局图片信息做全局特征提炼和学习
- CNN的filter完成工作后，生成一个matrix，是将所有局部小图片信息，分别做特征提炼和学习
，再做空间结构信息的还原整合的结果

**CNN中的padding算不算噪声？**
padding是增加各个边的pixels数量，目的是为了不丢弃原图信息，保持feature map的大小与原图一致，让更深层的layer的input依然保持有足够大的信息量。为了实现上述目的，却不做多余的事情，padding出来的pixel值都是0，不存在噪声问题。

**如何理解feature map在逐步缩小，feature map的channel在逐步增多？**
channels增多，是希望复杂特征的数量越多越好，越多我们对图片了解越多；
feature的尺寸越来越小，因为我们希望每个特征都是精炼简洁的，取出不必要的噪音；
每个特征，都是前一层feature map上的多个特征融合而成，本身的信息量充足，无需在长宽上增加信息量。

**如何理解GoogleNet inception结构设计的由来？**
用1x1,3x3,5x5 卷积并列组合设计，模拟sparse structure的效果。前面再加上1x1卷积层来实现dimension reduction+sparse的效果。
- 模型变深变宽，计算成本不显著增加
- 模型提炼的特征，具备scale invariance的能力

**深度神经网络设计中有哪些可以参考的点？**
设计参考：
1. 将5x5、7x7卷积核替换为更小的3x3卷积核
2. 1x1的卷积核是非常高效的
- 降维（ dimension reductionality ）。比如，一张500 * 500且厚度depth为100 的图片在20个filter上做1*1的卷积，那么结果的大小为500*500*20。再升维回到原始维度，可以大大减少参数量。
- 加入非线性。卷积层之后经过激励层，1*1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力；
3. 将NxN卷积核拆分为1xN和Nx1

**1*1的卷积核的作用：**
1. 实现跨通道的交互和信息整合
2. 进行卷积核通道数的降维和升维
3. 对于单通道feature map 用单核卷积即为乘以一个参数，而一般情况都是多核卷积多通道，实现多个feature map的线性组合

**数据增强的手段有哪些？**
1. 水平翻转
2. 随机裁剪、随机缩放
3. 变换（平移、旋转、stretching拉伸、shearing修剪……）

**梯度消失、爆炸的解决方案？**
神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是`梯度消失`。
产生的原因：
- 网络层数较深
- 采用不合适的损失函数，比如sigmoid

梯度爆炸与梯度消失类似，当每层的偏导数值都大于1时，经过多层的权重更新之后，梯度就会以指数形式增加，即为梯度爆炸。
产生原因：
- 网络层数较深
- 权重初始值太大
- 训练样本有误

【解决方法】
- 逐层训练加微调
该方法由 Geoffrey Hinton 于2006年提出，具体流程为每次只训练一层神经网络，待权重稳定之后，再使用该层网络的输出进行后一层网络的输入，重复该步骤至训练所有网络层。最后再对整个模型进行finetune，得到最优的模型。
- 梯度剪切
梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。
- 权重正则化
比较常见的是L1正则，和L2正则，在各个深度框架中都有相应的API可以使用正则化。
- relu、leakrelu、elu等激活函数
Relu: 如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。
- batchnorm
batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1，保证网络的稳定性。
- 残差结构
残差结构中的 shortcut 有效地避免了梯度消失与爆炸问题。
- LSTM
LSTM全称是长短期记忆网络（long-short term memory networks），是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)。

**为什么要用激活函数？**
激活函数是用来加入非线性因素的，因为非线性模型的表达能力不够，有些数据可以线性可分，而有些数据是线性不可分的，对于线性可分的数据就需要做线性变换，比如把x, y 轴变成x^2 * y^2等之类的变换；或者引入非线性函数。
`激活函数就是使神经网络具有拟合非线性函数的能力， 使神经网络具有强大的表达能力。`

**在合理的范围，增大batch_size有何好处？**
batch在神经网络本来是作为计算加速的，通过把数据进行统一大小，然后批量进入神经网络模型，以此到达加速的效果。但是batch_size是不能无限增大的。
1.  合适的batch大小可以内存的利用率，这个是必然的，大矩阵乘法的并行化效率提高。
2. 跑一次全数据集所需要的迭代次数减少了，时间成本可以节省了。
3. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。

但是，盲目增大batch_size也会带来以下后果：
1.  内存利用率提高了，但是内存容量可能撑不住
2. 跑一遍全数据集的迭代次数减少，要想达到同样的精度，所需要花费的时间大大增加了，从而对参数的修正就显得更加缓慢了。
3. batch_size 增加到一定的程度，其确定的下降方向已经基本不再变化。

**深度学习有哪些真实的应用程序？**
（1）Google一次性将文本翻译成数百种语言，通过一些应用于自然语言处理任务的深度学习模型实现。
（2）Siri、Alexa、Cortana等智能会话代理通过LSTM和RNN来简化语音识别技术，语音命令打开了一个全新的领域。
（3）计算机视觉领域的应用，例如OCR（光学字符识别）和实时语言翻译。
（4）Snapchat和Instagram等多媒体共享应用程序进行面部特征检测。
（5）医疗领域的应用，用来定位恶性细胞和其他异物，检测疾病。
……