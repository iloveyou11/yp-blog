---
title: CV系列1：计算机视觉基础知识
date: 2020-02-16
categories: CV
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

<!-- more -->

[CV系列1：计算机视觉基础知识](https://iloveyou11.github.io/2020/02/16/CV-01/)
[CV系列2：卷积神经网络的演变](https://iloveyou11.github.io/2020/02/20/CV-02/)
[CV系列3：卷积神经网络代码实现](https://iloveyou11.github.io/2020/03/01/CV-03/)
[CV系列4：目标检测算法](https://iloveyou11.github.io/2020/03/19/CV-04/)

#### 什么是深度学习
深度学习是指一类对具有深层结构的神经网络进行有效训练的方法。神经网络是一种由许多非线性计算单元（神经元）组成的分层系统，网络深度是不包括输入层的层数。

#### 1x1卷积
作用：
1. 可以增加模型的非线性性（相当于在不改变特征尺寸的基础上，额外引入了一个非线性层，使用非线性激活函数）
2. 进行特征降维，可以将多通道的特征图压缩到更小的channel（主要目的）

其中，1x1卷积的主要作用是为了特征降维，能极大地减少计算量，在GoogLeNet的Inception架构中，引入了1x1卷积来简化计算。

<img width="60%" src="https://i.loli.net/2020/04/06/HaqK5wDLAsPTEzC.jpg" alt="1x1卷积" />

以下是GoogLeNet的Inception结构，其中就应用到了1x1的卷积，极大地简化了计算量：

<img width="60%" src="https://i.loli.net/2020/04/06/isyzG24B3f5HYeA.jpg" alt="1x1卷积减少计算量" />

#### 空洞卷积

<img width="60%"  src="https://i.loli.net/2020/04/06/4jDPpI5Ub2o7Tne.jpg" alt="空洞卷积" />

#### 转置卷积（反卷积）

<img width="60%" src="https://i.loli.net/2020/04/06/ybJ71TREC8AloNU.jpg" alt="转置卷积" />

#### 什么是卷积神经网
卷积神经网络是一种特殊的多层感知机或前馈神经网络，具有局部连接、权值共享的特点，其中大量神经元按照一定方式组织起来对视野中的交叠区域产生反应。

卷积神经网络是深度学习中最为重要的模型。在2012年，提出了著名的AlexNet，在ImageNet（大规模图片分类竞赛）上取得了优异成绩，为深度学习奠定了基础。之后，卷积神经网络相继出现，如VGGNet、GoogLeNet、ResNet、SPPNet、DenseNet、Faster RCNN、YOLO、SSD、FCN、Mask RCNN、DCGAN等等，极大地推进了图片分类、识别和理解技术的发展。

#### 深度学习模型
- 卷积神经网络（CNN）
- 循环神经网络（RNN）
- 长短期记忆网络（LSTM）
- 深层自编码器（AE）
- 深层信念网络
- 强化学习网络（RLN）
- 生成式对抗网络（GAN）
- 受限玻尔兹曼机（RBM）
- 深层玻尔兹曼机（DSN）
……

#### 激活函数
常见激活函数有sigmoid、tanh、relu、leaky relu、maxout等等，它们的函数图像如下：
<img width="50%" alt="激活函数" src="https://i.loli.net/2020/03/23/PDkzl9BehuEnZ54.png"/>

**特点及问题：**
**1. Sigmoid**
- Sigmoid函数饱和使梯度消失。当神经元的激活在接近0或1处时会饱和，在这些区域梯度几乎为0，这就会导致梯度消失，几乎就有没有信号通过神经传回上一层。
- Sigmoid函数的输出不是零中心的。因为如果输入神经元的数据总是正数，那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降。

**2. tanh**
- Tanh解决了Sigmoid的输出是不是零中心的问题，但仍然存在饱和问题。
- 为了防止饱和，现在主流的做法会在激活函数前多做一步batch normalization，尽可能保证每一层网络的输入具有均值较小的、零中心的分布。

**3. ReLU**
- ReLU单元比较脆弱并且可能“死掉”，而且是不可逆的，因此导致了数据多样化的丢失。通过合理设置学习率，会降低神经元“死掉”的概率。

**4. Leaky ReLU**
- 能解决了ReLU神经元“死掉”的问题

**5. Maxout**
- Maxout是对ReLU和leaky ReLU的一般化归纳。Maxout具有ReLU的优点，如计算简单，不会 saturation，同时又没有ReLU的一些缺点，如容易go die。问题就是每个神经元的参数double（翻倍），这就导致整体参数的数量激增。

**6. Softmax**
- Softmax用于多分类神经网络输出，目的是让大的更大。

#### 优化器（梯度下降算法）
- **随机梯度下降（SGD）**
分为两种模式：整体梯度下降、mini梯度下降。**整体梯度下降**是先把所有样本随机洗牌，再逐一计算每个样本对梯度的贡献去更新权值。缺点是梯度下降的过程不太稳定、波动较大。**mini梯度下降**是将所有样本随机洗牌后分为若干大小为m的块，再逐一计算每块对梯度的贡献去更新权值。
- **Momentum动量**
加一个动量（借助物理学的惯性原理）

<img width="50%" src="https://i.loli.net/2020/03/23/EP24GoBwSs8jNlx.jpg" alt="Momentum动量"/>

- **Adagrad**
在不同方向上的学习率是可以变化的，计算时除了这个方向上所有之前梯度的平均值。

<img width="50%" src="https://i.loli.net/2020/03/23/A5ougkDGOqzniVy.jpg" alt="Adagrad">

- **RMSProp**
在不同方向上的学习率是可以变化的，计算时除了这个方向上所有之前梯度的加权平均值（最近的gradient权重大，过去的gradient权重小）

<img width="50%" src="https://i.loli.net/2020/03/23/EB9vHFjotcremDW.jpg" alt="RMSProp">

- **Adam**
记录过去一段时间的梯度平方和（类似Adagrad和RMSProp）以及梯度的和（类似Momentum动量），把优化看作是铁球滚下山坡，定义了一个带动量和摩擦的铁球。Adam是目前最好的算法，在不知道如何选择时就选择它。

#### 权值(w)偏置(b)初始化
在训练神经网络之前，必须对其权值和偏置进行初始化，常用的初始化方法有3种：高斯初始化、Xavier初始化和MSRA初始化。它们一般都是把偏置初始化为0，对权值进行随机初始化。

#### 过拟合和欠拟合
**1. 如何防止过拟合**
- 获取和使用更多的数据（数据集增强）——解决过拟合的根本性方法
- 采用合适的模型（控制模型的复杂度）
- 降低特征的数量
- 使用正则化项（常见的L0、L1、L2正则化项，L1可以稀疏神经元，L2可以降低神经元权重，从而降低了过拟合的风险）
- dropout（在网络训练过程中让网络某些节点不工作，不过度依赖某些神经元）
- early stopping（如果发现随着训练迭代次数的增多，测试集并没有出现损失减少反而上升的现象，则可能是发生了过拟合，导致了泛化能力降低，应提前停止训练）

**2. 如何防止欠拟合**
- 增加网络复杂度
- 在模型中增加特征

---

#### 为什么需要激活函数？
如果不适用激活函数，那么就算层数再深，表达的也只是线性函数，和使用一层网络表达的效果相同。

#### 网络加深就一定好吗？
加深在一定程度上可以提升模型性能，但是未必就是网络越深越越好。深层网络带来的梯度不稳定，网络退化的问题始终都是存在的，可以缓解，没法消除。这就有可能出现网络加深，性能反而开始下降。深层网络的训练很少能突破30层，VGGNet19层，GoogleNet22层，MobileNet28层，经典的网络超过30层的也就是ResNet系列常见的ResNet50，ResNet152了。ResNet网络使用跳连结构，有效地解决了梯度消失问题。

模型加深还可能出现的一些问题是导致某些浅层的学习能力下降，限制了深层网络的学习，这也是跳层连接等结构能够发挥作用的很重要的因素。