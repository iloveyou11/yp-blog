---
title: CS224n课程笔记整理
date: 2020-08-06
categories: NLP
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

**课程搬运地址：**
[课程主页](http://web.stanford.edu/class/cs224n/index.html)
[官方课程视频网站](http://onlinehub.stanford.edu/cs224)
[官方油管视频List](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)
[课程除视频以为的相关资料都可以从schedule下载，包括ppt等](http://web.stanford.edu/class/cs224n/index.html#schedule)
[课程优秀项目网站](http://web.stanford.edu/class/cs224n/project.html)
[B站——CS224n 斯坦福深度自然语言处理课](https://www.bilibili.com/video/BV1pt411h7aT)
[B站——斯坦福CS224n深度学习自然语言处理课程](https://www.bilibili.com/video/BV1Eb411H7Pq)

### 01-02 Word Vectors
词向量 `word vectors` 有时被称为词嵌入 `word embeddings` 或词表示 `word representations`，它们是分布式表示 `distributed representation`。

自然语言处理有不同层次的任务，从语言处理到语义解释再到语篇处理：
1. Easy
- 拼写检查 Spell Checking
- 关键词检索 Keyword Search
- 同义词查找 Finding Synonyms
2. Medium
- 解析来自网站、文档等的信息
3. Hard
- 机器翻译 Machine Translation
- 语义分析 Semantic Analysis
- 指代消解 Coreference
- 问答系统 Question Answering

**两类词向量计算方法：**
1. 基于统计并且依赖矩阵分解（例如LSA，HAL），能有效利用全局信息
2. 通过局部上下文信息来预测词向量

#### CBOW、skip-gram
介绍一下非常有效的概率模型：Word2vec。包括了：
- `算法`：`CBOW`（根据中心词周围的上下文单词来预测该词的词向量）和`skip-gram`（根据中心词预测周围上下文的词的概率分布）模型
- `训练方法`：`negative sampling` 和 `hierarchical softmax`，`negative sampling`是负采样，`hierarchical softmax`是使用层次树结构来计算所有词的概率。

> 视频详细介绍了`CBOW`和`skip-gram`模型，`negative sampling`和`hierarchical softmax`训练方法，具体讲解请观看视频内容

工具包：`Gensim`，Gensim提供了将 Glove 转化为Word2Vec格式的API

#### GloVe

Glove 利用全局统计量，以最小二乘为目标，预测单词`i`出现在单词`j`上下文中的概率。

> 视频详细介绍了`GloVe`模型，具体讲解请观看视频内容

### 03 Word Window Classification,Neural Networks, and Matrix Calculus

#### Named Entity Recognition (NER)
`NER`是命名实体识别，查找和分类文本中的名称，可能的用途有：
- 跟踪文档中提到的特定实体（组织、个人、地点、歌曲名、电影名等）
- 对于问题回答，答案通常是命名实体
- 许多需要的信息实际上是命名实体之间的关联
- 同样的技术可以扩展到其他 slot-filling 槽填充 分类

但是！`NER`存在一些难点：
- 很难计算出实体的边界
- 很难知道某物是否是一个实
- 很难知道未知/新奇实体的类别
- 实体类是模糊的，依赖于上下文

#### 神经网络

> 视频详细介绍一些单层和多层神经网络，以及如何将它们用于分类目的。讨论一些训练神经网络的实用技巧和技巧，包括:神经元单元(非线性)、梯度检查、Xavier参数初始化、学习率、Adagrad等。最后,我们将鼓励使用递归神经网络作为语言模型。

### 04 Backpropagation and Computation Graphs

> 视频详细介绍了反向传播的过程

### 05 Linguistic Structure Dependency Parsing
#### 依存语法
依存语法是给定一个输入句子S，分析句子的句法依存结构的任务。依存句法的输出是一棵依存语法树，其中输入句子的单词是通过依存关系的方式连接。正式地，依存语法问题是创建一个输入句子的单词 S=w0w1w2...wn 到它的依存语法树的映射图G，最近几年提出了很多以依存句法为基础的的变体，包括基于神经网络的方法。

> 视频详细介绍了语法结构、依存关系语法分析（依存分析）

### 06 The probability of a sentence Recurrent Neural Networks and Language Models
介绍一个新的神经网络家族Recurrent Neural Networks (RNNs)
1. RNN的优点
- 可以处理任意长度的输入
- 步骤 t 的计算(理论上)可以使用许多步骤前的信息
- 模型大小不会 随着输入的增加而**增加**
- 在每个时间步上应用相同的权重，因此在处理输入时具有对称性
2. RNN的缺点
- 递归计算速度慢
- 在实践中，很难从**许多步骤前**返回信息
- 后面的课程中会详细介绍

> 视频详细讲解了RNN模型

### 07 Vanishing Gradients and Fancy RNNs
课程大纲：
```
梯度消失问题 →两种新类型RNN：LSTM和GRU
其他梯度消失（爆炸）的解决方案
  - Gradient clipping
  - Skip connections
更多花哨的RNN变体
  - 双向RNN
  - 多层RNN
```

> 视频详细讲解了RNN变种模型，如GRU、LSTM、双向RNN、多层RNN等等

### 08 Machine Translation, Sequence-to-sequence and Attention
课程大纲：
```
引入新任务：机器翻译
引入一种新的神经结构：
  sequence-to-sequence
  机器翻译是sequence-to-sequence的一个主要用例
引入一种新的神经技术：注意力
  sequence-to-sequence通过attention得到提升
```

> 课程详细讲解了`sequence-to-sequence`模型和`attention`注意力机制

### 09 Practical Tips for Final Projects

> 课程详细讲解了工程化项目时，项目流程、需要注意的点、小技巧等等

### 10 Question Answering and the Default Final Project

> 课程详细讲解了问答系统的算法及搭建

### 11 ConvNets for NLP

> 课程详细讲解了CNN网络在NLP任务中的应用

### 12 Information from parts of words Subword Models

> 课程介绍了Subword Model、fastText

### 13 Modeling contexts of use Contextual Representations and Pretraining

> 课程介绍了`Transformers`、`BERT`及其在NLP任务中的应用

### 14 Transformers and Self-Attention For Generative Models

> 课程介绍了`Self-Attention`的原理以及在生成式模型中的运用

### 15 Natural Language Generation

> 课程介绍了NLG tasks（NLG：Natural Language Generation）
自然语言生成指的是我们生成（即写入）新文本的任何设置

NLG 包括以下任务：
- 机器翻译
- 摘要
- 对话（闲聊和基于任务）
- 创意写作：讲故事，诗歌创作
- 自由形式问答（即生成答案，从文本或知识库中提取）
- 图像字幕

### 16 Coreference Resolution
Coreference Resolution是“指代消解”任务，识别所有涉及到相同现实世界实体。

### 17 Multitask Learning
1. 多任务学习是一般NLP系统的阻碍
2. 统一模型可以决定如何转移知识（领域适应，权重分享，转移和零射击学习）
3. 统一的多任务模型可以
- 更容易适应新任务
- 简化部署到生产的时间
- 降低标准，让更多人解决新任务
- 潜在地转向持续学习

### 18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment
### 19 Safety, Bias, and Fairness
### 20 The Future of NLP + Deep Learning
1. 利用无标签数据
- Back-translation 和 无监督机器翻译
- 提高预训练和GPT-2
2. 接下来呢？
- NLP技术的风险和社会影响
- 未来的研究方向