---
title: NLP任务-10-自动补全
date: 2020-06-20
categories: NLP
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

[NLP任务-01-词嵌入](https://iloveyou11.github.io/2020/06/11/NLP%E4%BB%BB%E5%8A%A1-01-%E8%AF%8D%E5%B5%8C%E5%85%A5/)
[NLP任务-02-序列标注](https://iloveyou11.github.io/2020/06/12/NLP%E4%BB%BB%E5%8A%A1-02-%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/)
[NLP任务-03-分词](https://iloveyou11.github.io/2020/06/13/NLP%E4%BB%BB%E5%8A%A1-03-%E5%88%86%E8%AF%8D/)
[NLP任务-04-词性标注](https://iloveyou11.github.io/2020/06/14/NLP%E4%BB%BB%E5%8A%A1-04-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/)
[NLP任务-05-命名实体识别](https://iloveyou11.github.io/2020/06/15/NLP%E4%BB%BB%E5%8A%A1-05-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%ABNER/)
[NLP任务-06-依存句法分析](https://iloveyou11.github.io/2020/06/16/NLP%E4%BB%BB%E5%8A%A1-06-%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/)
[NLP任务-07-信息抽取](https://iloveyou11.github.io/2020/06/17/NLP%E4%BB%BB%E5%8A%A1-07-%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/)
[NLP任务-08-文本分类](https://iloveyou11.github.io/2020/06/18/NLP%E4%BB%BB%E5%8A%A1-08-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/)
[NLP任务-09-文本聚类](https://iloveyou11.github.io/2020/06/19/NLP%E4%BB%BB%E5%8A%A1-09-%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB/)
[NLP任务-10-自动补全](https://iloveyou11.github.io/2020/06/20/NLP%E4%BB%BB%E5%8A%A1-10-%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8/)
[NLP任务-11-语义消歧](https://iloveyou11.github.io/2020/06/21/NLP%E4%BB%BB%E5%8A%A1-11-%E8%AF%AD%E4%B9%89%E6%B6%88%E6%AD%A7/)

自动补全是个基于NLP的搜索任务，主要包括了以下几个方面：
1.	语言理解
2.	文档召回和排序
3.	文本生成辅助搜索

### 语言理解
这块的任务都是以分类问题为准，所谓的理解，只是把语言抽象到某一个理解空间，将其进行标准化，以便进行批量化处理。主要步骤是：
1. 实体标注(即命名实体识别)
2. 实体消歧或指代消歧(基于知识的预测)
3. 意图识别(句子级别预测，甚至就是简单的文本分类)

#### 命名实体识别（NER）
`实体标注方面`，传统的统计方法是HMM(隐马尔可夫)、MEMM(最大熵马尔可夫)、CRF(条件随机场)，基本就和命名实体识别类似了，而在深度学习引入后，形成了输入层、编码层、解码层的主要架构，同过预训练表征模型(如w2v)、深度学习结构(CNN、RNN等)以及输出层(CRF、softmax)等结构链接，完成最基本的结构。

**数据标注方式：**
主要有BIO和BIOES两种。BIOES如下：
```
BIOES如下：

B，即Begin，表示开始
I，即Intermediate，表示中间
E，即End，表示结尾
S，即Single，表示单个字符
O，即Other，表示其他，用于标记无关字符

BIO如下：

B，即Begin，表示开始
I，即Intermediate，表示中间
E，即End，表示结尾
```

##### HMM(隐马尔可夫)
**具体流程：**

<img src="https://i.loli.net/2020/07/23/MPQpLulqTESiIna.png" alt="HMM参数" width="80%" />

当获得了分好词的语料之后，三个概率`θ=(A,B,Π)`可以通过如下方式获得：
(1) 初始状态概率`Π`-`P(z1)`
统计每个句子开头，序列标记分别为B，S的个数，最后除以总句子的个数，即得到了初始概率矩阵。
(2) 状态转移概率`A`-`(zi|zi-1)`
根据语料，统计不同序列状态之间转化的个数，例如`count(yi=”E”|yi-1=”M”)`为语料中i-1时刻标为“M”时，i时刻标记为“E”出现的次数。得到一个`4*4`的矩阵，再将矩阵的每个元素除以语料中该标记字的个数，得到状态转移概率矩阵。
(3) 输出观测概率`B`-`P(xi|zi)`
根据语料，统计由某个隐藏状态输出为某个观测状态的个数，例如`count(xi=”深”|yi=”B”)`为i时刻标记为“B”时，i时刻观测到字为“深”的次数。得到一个`4*N`的矩阵，再将矩阵的每个元素除以语料中该标记的个数，得到输出观测概率矩阵。

训练结束后，即可获得三个概率矩阵`θ=(A,B,Π)`，接下来需要使用维特比算法获得一个句子的最大概率分词标记序列。

<img src="https://i.loli.net/2020/07/27/qPQNLlFvnzTyUHr.png" alt="NER-HMM" width="80%" />

```
第一个词为“我”，通过初始概率矩阵和输出观测概率矩阵分别计算delta1("B")=P(y1=”S”)P(x1=”我”|y1=”S”)，delta1("M")=P(y1=”B”)P(x1=”我”|y1=”B”)，delta1("E")=P(y1=”M”)P(x1=”我”|y1=”M”)，delta1("S")=P(y1=”E”)P(x1=”我”|y1=”E”)，并设kethe1("B")=kethe1("M")=kethe1("E")=kethe1("S")=0；
同理利用公式分别计算：
delta2("B")，delta2("M")，delta2("E")，delta2("S")。图中列出了delta2("S")的计算过程，就是计算：
P(y2=”S”|y1=”B”)P(x2=”爱”|y2=”S”)
P(y2=”S”|y1=”M”)P(x2=”爱”|y2=”S”)
P(y2=”S”|y1=”E”)P(x2=”爱”|y2=”S”)
P(y2=”S”|y1=”S”)P(x2=”爱”|y2=”S”)
其中P(y2=”S”|y1=”S”)P(x2=”爱”|y2=”S”)的值最大，为0.034，因此delta2("S")，kethe2("S")="S"，同理，可以计算出delta2("B")，delta2("M")，delta2("E")及kethe2("B")，kethe2("M")，kethe2("E")。

同理可以获得第三个和第四个序列标记的delta和kethe。
到最后一个序列，delta4("B")，delta4("M")，delta4("E")，delta4("S")中delta4("S")的值最大，因此，最后一个状态为”S”。
最后，回退，
i3 = kethe4("S") ="B"
i2 =kethe3("B") = "S"
i1 = kethe2("S") ="S"
求得序列标记为：“SSBE”。
```

**HMM解决序列标注问题的优势与不足：**
HMM时非常适合用于序列标注问题，但HMM引入了马尔科夫假设，即T时刻的状态仅仅与前一时刻的状态相关。但是，语言往往是前后文相互照应的，所以HMM可能会有它的局限和问题，我们可以思考一下，如何解决这个问题。

##### CRF(条件随机场)
NER任务特征提取的网路结构如下：

<img src="https://i.loli.net/2020/07/27/7sqMkygxn83UYJO.png" alt="NER-CRF" width="80%" />

句子经过双向LSTM进行特征提取之后，会得到一个特征输出。训练时，将这个特征和对应的label输入到条件随机场中，就可以计算损失了。预测时，将自然语言输入到该网络，经CRF就可以识别该句子中的实体了。

`条件随机场(CRF)在现今NLP中序列标记任务中是不可或缺的存在。太多的实现基于此，例如LSTM+CRF，CNN+CRF，BERT+CRF。因此，这是一个必须要深入理解和吃透的模型。！！`

##### LSTM+CRF
采用LSTM作为特征抽取器，再接一个CRF层来作为输出层，结构如下图所示：

<img src="https://i.loli.net/2020/07/27/aBxdrj6Nos7SOWK.png" alt="NER-LSTM+CRF" width="80%" />

##### CNN+CRF
采用LSTM作为特征抽取器，再接一个CRF层来作为输出层，结构如下图所示：

<img src="https://i.loli.net/2020/07/27/fu8t9FBAh6yQCHb.png" alt="NER-CNN+CRF" width="80%" />

虽然CNN并不太擅长长序列的特征提取，但是CNN具有非常高效的并行运算能力，能够加快运算速度。

##### BERT+（LSTM）+CRF
利用预训练好的BERT模型，再用少量的标注数据进行fine tune，能够快速地实现NER任务。

<img src="https://i.loli.net/2020/07/27/MC2DtKFon9jUhPz.png" alt="NER-BERT+CRF" width="80%" />

#### 指代消歧
`实体消歧或指代消歧`主要是解决在用户搜索的语句中出现的问题，例如"苹果"到底是水果还是手机等等，这个是依赖上下文信息和知识库合力完成的，例如一句话"我爱吃苹果"，这个"吃"其实就是一个上下文的信息，另一方面我们要通过这个"吃"推断出这个水果的含义，我们就需要借助知识库。

##### 基于二元分类的方法
共指消解需要考虑的特征主要分为以下几类：`词汇、距离、一致性、语法、语义等`。
1. 词汇特征主要考虑两个 Mention 的字符串的匹配程度，一般而言字符串相同程度越高的 Mention共指概率越大。
2. 距离特征主要考察两个 Mention 的句子距离，这个主要依据是共指事实上也是一种局部性的替代关系， 越是临近的 Mention 之间共指概率越大。 一般而言，两个 Mention 相隔超过三个句子，共指的可能性就会很小了。
3. 一致性特征详细可以分为性别、单复数、语义类别等是否一致。这组特征主要起到筛选的作用。
4. 语法关系用来判断两个 Mention 的语法角色之间的关系，由于对句子深层的语法分析还很难办到，这里主要采用的是一些基于特定模板的方法，例如判断两个 Mention 之间是否被逗号格开或者相邻等来决定是否具有同位关系。
5. 语义特征主要是考察两个 Mention 在语义类别不一致时是否满足上下位或者同义、近义关系。这种判断主要依赖于具体的语言学词典，例如英文上的 WordNet(Fellbaum, 1998)、中文上的 HowNet(董振东,董强, 2001)等。

##### 端到端的神经共指消解
> 参考论文：Lee K, He L, Lewis M, et al. End-to-end Neural Coreference Resolution[J]. 2017:188-197.

<img src="https://i.loli.net/2020/07/27/smUq8pnOEoQVzPk.png" alt="指代消解" width="80%" />

【具体步骤】
1. 计算每个span的向量表示，并以此对各潜在mention(同一实体)打分。具体的做法是：将编码信息切分成一组sentence，对每一个sentence独立地构建深度学习模型，将特征矩阵输入到深度学习模型（如LSTM、CNN）中，得到由sentence构成的篇章文本中每一个词的向量表示。对于每个span，将其中的每个词进行组合得到span的向量表示。然后对span的向量表示进行非线性映射，得到每个潜在mention的分数，并以该分数大小对mention进行修剪，得到一定数量的mention。
2. 对每一对span的向量表示计算先行语得分。通过对两个span的mention score及它们的配对先行语得分求和，得到一对span最终的共指得分。

#### 文本分类
`意图识别方面`，应该是这几块里面最简单的，就是一套深度学习，框架即可完成，fasttext、CNN以及BiRNN-Attention。这里详见[NLP项目专题-01]()

### 文档召回和排序
这个思路和推荐系统类似，我们先把有关的全都拿出来，然后再用更为精细的方法排好序展示给用户，此处就有两个大步，召回和排序。
#### 文档召回
`召回方面`，要求更全，此处又有句法召回和语义召回。句法召回说白了就是匹配，但这里面的学问可是非常多的，字符串匹配、倒排索引(搜索系统中非常关键的基础知识)、多路召回(多领域)，语义召回则是通过词向量近邻等方式扩大召回的内容。

**倒排表（Inverted Index）**：有一个完整的词典库，分别记录每个单词出现在哪些文档中，例如：
```
我们：[Doc1，Doc13]
昨天：[Doc2]
在：[Doc1，Doc4，Doc5]
运动：[Doc1，Doc3，Doc5]
什么：[Doc1，Doc6]
```
这样可以快速找到哪个单词出现在哪个文档中（否则根据单词一个个去搜索文档时间复杂度非常高）

#### 文档排序
`排序方面`，LTR(learningtorank)其实是一个隐含在暗线但实际上已经非常经典的方向，就是为了研究排序的。
##### TextRank
TextRank是一种文本排序算法，是由网页排序算法PageRank发展而来。TextRank算法是利用局部词汇之间关系（共现窗口）对后续关键词进行排序，直接从文本本身抽取。TextRank可以进行文档排序、关键词提取、文本摘要提取等等。
##### 其他
要了解更多关于搜索引擎的排序算法，可参考[《回顾·搜索引擎算法体系简介——排序和意图篇》](https://cloud.tencent.com/developer/news/184638)
如果是视频搜索排序，可以参考[《阿里文娱搜索算法实践与思考》](https://www.infoq.cn/article/RUlwIBXPmUKILgqiyR4I)

<img src="https://i.loli.net/2020/07/27/WUANVdqfYw28HPs.png" alt="排序算法" width="80%" />

### 文本生成辅助搜索
这块主要用于辅助用户进行搜索，主要体现在下面三块功能上：
1. 自动补全。很好理解，大家在很多搜索引擎中都会看到，在百度下输入"自然语言"，他能给你预测出你可能要搜"自然语言处理"。这个使用js就可以直接解决。
2. 搜索重构。举个例子吧，你输入的是吃鸡，实际上文档库的标题是"和平精英"，那要映射过去，其实就是一种同义词重构。这个使用同义词查询就可以解决。
3. 拼写修正。英文有拼写问题，中文有错别字问题，不能保证用户100%输入正确，平时打字都可能手滑，为了更准确理解语义，我们必须在进行语义分析前修正这些错误。这个可以采用朴素贝叶斯方法解决。