---
title: CV系列5：目标检测算法
date: 2020-03-19
categories: CV
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

核心内容：传统目标检测算法（Viola-Jones、HOG+SVM、DPM），two-stage系列（R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN、Mask R-CNN），one-stage系列（YOLO系列、SSD等）

<!-- more -->

[CV系列1：计算机视觉基础知识](https://iloveyou11.github.io/2020/02/16/CV-01/)
[CV系列2：卷积神经网络的演变](https://iloveyou11.github.io/2020/02/20/CV-02/)
[CV系列3：卷积神经网络代码实现](https://iloveyou11.github.io/2020/03/01/CV-03/)
[CV系列4：目标检测算法](https://iloveyou11.github.io/2020/03/19/CV-04/)

## 什么是目标检测
如何理解一张图片？根据后续任务的需要，有三个主要的层次：

<img width="60%" src="https://i.loli.net/2020/03/25/fhDLCnFHlgGWmOU.jpg" alt="检测任务分类" />

1. 一是`分类（Classification）`，即是将图像结构化为某一类别的信息，用事先确定好的类别(string)或实例ID来描述图片。这一任务是最简单、最基础的图像理解任务，也是深度学习模型最先取得突破和实现大规模应用的任务。其中，ImageNet是最权威的评测集，每年的ILSVRC催生了大量的优秀深度网络结构，为其他任务提供了基础。在应用领域，人脸、场景的识别等都可以归为分类任务。ß
2. 二是`检测（Detection）`。分类任务关心整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标ß，要求同时获得这一目标的类别信息和位置信息。相比分类，检测给出的是对图片前景和背景的理解，我们需要从背景中分离出感兴趣的目标，并确定这一目标的描述（类别和位置），因而，检测模型的输出是一个列表，列表的每一项使用一个数据组给出检出目标的类别和位置（常用矩形检测框的坐标表示）。
3. 三是`分割（Segmentation）`。分割包括`语义分割（semantic segmentation）`和`实例分割（instance segmentation）`，前者是对前背景分离的拓展，要求分离开具有不同语义的图像部分，而后者是检测任务的拓展，要求描述出目标的轮廓（相比检测框更为精细）。分割是对图像的像素级描述，它赋予每个像素类别（实例）意义，适用于理解要求较高的场景，如无人驾驶中对道路和非道路的分割。
## 目标检测算法

<img width="80%" src="https://i.loli.net/2020/03/25/Si2FBakzhrI9l4v.jpg" alt="目标检测算法" />

**传统目标检测算法 VS 基于深度学习的目标检测算法：**
传统目标检测算法：
- 手动设计特征
- 滑动窗口
- 传统分类器
- 多步骤
- 准确度和实时性差

基于深度学习的目标检测算法：
- 深度网络学习特征
- proposal或者直接回归
- 深度网络
- 端到端
- 准确度高和实时性好

## 模型指标评价

**判断目标检测结果好坏的指标：**
1. 模型检测速度fps
2. mAP—平均准确率均值

<img width="50%"  src="https://i.loli.net/2020/04/06/MQ7x8EfZk9JiBta.jpg" alt="mAP" />

计算方法：

<img width="50%" src="https://i.loli.net/2020/04/06/oTMuY25gpdIaeGf.jpg" alt="mAP计算方法" />

**one-stage vs two stage的目标检测算法：**

<img width="50%" src="https://i.loli.net/2020/04/07/182xYpwVW9jXGq6.jpg" alt="one and two" />

<img width="50%" src="https://i.loli.net/2020/04/07/3cwfBMvyaSDolb5.jpg" alt="one and two效果图" />

## 传统目标检测算法

原理：滑动窗口法+传统机器学习分类器

<img width="50%" src="https://i.loli.net/2020/04/06/25etcPVKwn1z7gb.jpg" alt="传统目标检测" />

缺点：
1. 识别效果不够好，准确率不高
2. 计算量及较大，运行速度慢
3. 可能会出现多个正确识别的结果
算法基本流程如下：

<img width="80%" src="https://i.loli.net/2020/03/25/ok12gCjlupOwvXd.jpg" alt="传统目标检测流程" />

### Viola-Jones算法
VJ算法，用于人脸检测
- Haar特征抽取
- 训练人脸分类器（采用Adaboost算法，在训练同时不停更新样本权重）
- 基于滑动窗口

以下是Haar特征，采用差分运算求解梯度，value=白-黑

<img width="40%" src="https://i.loli.net/2020/03/25/4EGZT7HJVlg3SyD.jpg" alt="Haar特征" />

Adaboost算法流程：
- 初始化样本的权重w，样本权重之和为1
- 训练弱分类器
- 更新样本权重
- 循环第2步—训练弱分类器
- 结合多个分类器结果，进行投票

<img width="40%" src="https://i.loli.net/2020/03/25/HRrfJgAQTGVi6St.jpg" alt="Adaboost" />

### HOG+SVM算法
HOG+SVM算法主要用于行人检测，基本流程如下：
- 提取HOG特征
- 训练SVM分类器
- 利用滑动窗口提取目标区域，进行分类判断
- NMS
- 输出检测结果

HOG特征维度很高，通常需要降维，以下是HOG特征的求解步骤：

<img width="50%" src="https://i.loli.net/2020/03/25/mVxJHAYRfdezycr.jpg" alt="HOG特征" />

- 灰度化+Gamma变换
- 计算梯度map
- 图像划分为小的cell，统计每个cell梯度直方图
- 多个cell组成一个block，特征归一化
- 多个block串联并归一化

### DPM算法
DPM算法是对HOG特征的扩展，常用于物体检测，传统目标检测算法的巅峰之作。

### FCN
原理：滑动窗口法+CNN提升识别的准确率
解决了传统目标检测中需要针对每个滑动窗口计算分类的效率问题，使用FCN可以一次性计算。

<img width="50%" src="https://i.loli.net/2020/04/06/R81rLtlbj4uPNm2.jpg" alt="FCN" />

<img width="50%" src="https://i.loli.net/2020/04/06/YHGECgKUhsVJAjf.jpg" alt="FCN2" />


## 深度学习目标检测算法

传统的计算机视觉方法常用精心设计的手工特征(如SIFT, HOG)描述图像，而深度学习的方法则倡导习得特征，从图像分类任务的经验来看，CNN网络自动习得的特征取得的效果已经超出了手工设计的特征。

### two-stage

<img width="80%" src="https://i.loli.net/2020/03/25/TcyFGvO2rZa1zYD.jpg" alt="two-stage常用算法" />

**two-stage核心组件：**
1）CNN网络
- 从简到繁再到简的卷积神经网络
- 多尺度特征融合的网络
- 更轻量级的CNN网络
2）RPN网络
- 区域推荐（Anchor机制）
- ROI pooling
- 分类和回归

**two-stage改进方向：**
- 更好的网络特征
- 更精准的RPN
- 更完美的ROI分类
- 样本后处理
- 更大的mini-batch

#### R-CNN
R-CNN将检测抽象为两个过程，一是基于图片提出若干可能包含物体的区域（即图片的局部裁剪，被称为Region Proposal），如Selective Search算法；二是在提出的这些区域上运行当时表现最好的分类网络（AlexNet），得到每个区域内物体的类别。

一是数据的准备。输入CNN前，我们需要根据Ground Truth对提出的Region Proposal进行标记，这里使用的指标是IoU（Intersection over Union，交并比）。IoU计算了两个区域之交的面积跟它们之并的比，描述了两个区域的重合程度。

另一点是位置坐标的回归（Bounding-Box Regression），这一过程是Region Proposal向Ground Truth调整，实现时加入了log/exp变换来使损失保持在合理的量级上，可以看做一种标准化（Normalization)操作。

<img width="80%" src="https://i.loli.net/2020/03/25/htX6erfwgaQuJFC.jpg" alt="R-CNN" />

**R-CNN的想法直接明了，即将检测任务转化为区域上的分类任务，是深度学习方法在检测任务上的试水。模型本身存在的问题也很多，如需要训练三个不同的模型（proposal, classification, regression）、重复计算过多导致的性能问题等。**

<img width="60%" src="https://i.loli.net/2020/04/06/Zh6Gejk9K4rL8pl.jpg" alt="RCNN" />

<img width="60%" src="https://i.loli.net/2020/04/06/ycnjhArmeJkYKVx.jpg" alt="RCNN算法流程" />

**具体算法流程如下：**
1. 选择一个分类模型（比如AlexNet、VGGNet等）
2. 去掉最后一个全连接层，将分类数从1000改为N+1（注意这里+1是添加了背景分类，N值根据实际业务场景选定），对该模型做fine-tuning（主要目的是优化卷积层和池化层的参数）

<img width="60%" src="https://i.loli.net/2020/04/06/CtF87aJjST2fZeL.jpg" alt="RCNN流程分析" />

3. 对每个候选区域进行特征提取，resize区域大小，将第5个池化层的输出（即候选区提取到的特征）保存到硬盘

<img width="60%" src="https://i.loli.net/2020/04/06/gfpMmtRDnYe4b9G.jpg" alt="RCNN流程分析2" />

4. 使用pooling5输出的图像特征训练SVM分类器，判断此候选区内的物体类别。每个类别对应一个SVM
5. 使用pooling5输出的图像特征训练一个回归器（dx，dy，dw，dh）。其中，dx是水平平移，dy是垂直平移，dw是宽度缩放，dh是高度缩放。在测试阶段，使用回归器调整候选框位置。

#### SPP-Net

<img width="60%" src="https://i.loli.net/2020/04/06/e32O86LCG7JFTsM.jpg" alt="SPP" />

SPP-Net针对RCNN做了改进，重点是：
1. 对原图只做一次卷积操作，就可以得到每个候选区域的特征。而不是类似RCNN对2000个候选区分别做CNN。
2. 使用了金字塔池化层技术，保证了原始候选区不管什么形状都可以得到同样尺寸的输出，而不是类似RCNN中将每个不同尺寸的候选区调整至相同大小。（4x4+2x2=20个值）

<img width="60%" src="https://i.loli.net/2020/04/06/tY2pAHr5amG1zd4.jpg" alt="金字塔池化层" />

经过实验证明，SPP-Net图像检测速度比RCNN提高了约100倍。

#### Fast R-CNN
R-CNN耗时的原因是CNN是在每一个Proposal上单独进行的，没有共享计算，便提出将基础网络在图片整体上运行完毕后，再传入R-CNN子网络，共享了大部分计算，故有Fast之名。

<img width="80%" src="https://i.loli.net/2020/03/25/FBNE84oVOxqXWhY.jpg" alt="Fast R-CNN" />

图片经过`feature extractor`得到`feature map`, 同时在原图上运行`Selective Search`算法并将`RoI`（Region of Interset，实为坐标组，可与Region Proposal混用）映射到到`feature map`上，再对每个RoI进行RoI Pooling操作便得到等长的`feature vector`，将这些得到的`feature vector`进行正负样本的整理（保持一定的正负样本比例），分batch传入并行的`R-CNN`子网络，同时进行分类和回归，并将两者的损失统一起来。

`RoI Pooling` 是对输入`R-CNN`子网络的数据进行准备的关键操作。我们得到的区域常常有不同的大小，在映射到`feature map`上之后，会得到不同大小的特征张量。`RoI Pooling`先将RoI等分成目标个数的网格，再在每个网格上进行`max pooling`，就得到等长的`RoI feature vector`。

**Fast R-CNN的这一结构正是检测任务主流two-stage方法所采用的元结构的雏形，将Proposal, Feature Extractor, Object Classification&Localization统一在一个整体的结构中，并通过共享卷积计算提高特征利用效率。**

<img width="60%" src="https://i.loli.net/2020/04/06/AKcEBDu3avneIr5.jpg" alt="Fast RCNN" />

Fast RCNN针对SPP-Net做了改进，重点是：
1. 使用了ROI Pooling（是金字塔池化层的一种简单化的形式）

<img width="60%" src="https://i.loli.net/2020/04/06/EPS5q7tFzWAxr8N.jpg" alt="ROI Pooling" />

2. 将回归放进了神经网络内部，作为了一个整体，而不是单独地训练回归器和分类器

<img width="60%" src="https://i.loli.net/2020/04/06/478TSbjWtCmoLD3.jpg" alt="Fast RCNN对比" />

#### Faster R-CNN
Faster R-CNN是two-stage方法的奠基性工作，提出的`RPN网络`取代`Selective Search`算法使得检测任务可以由神经网络`端到端`地完成。粗略的讲，`Faster R-CNN = RPN + Fast R-CNN`，跟RCNN共享卷积计算的特性使得RPN引入的计算量很小，使得Faster R-CNN可以在单个GPU上以5fps的速度运行，而在精度方面达到SOTA（State of the Art，当前最佳）。

<img width="80%" src="https://i.loli.net/2020/03/25/M8Fe1urqchszfLb.jpg" alt="Faster R-CNN" />

第一步是在一个滑动窗口上生成不同大小和长宽比例的anchor box，取定IoU的阈值，按Ground Truth标定这些anchor box的正负。于是，传入RPN网络的样本数据被整理为anchor box（坐标）和每个anchor box是否有物体（二分类标签）。RPN网络将每个样本映射为一个概率值和四个坐标值，概率值反应这个anchor box有物体的概率，四个坐标值用于回归定义物体的位置。最后将二分类和坐标回归的损失统一起来，作为RPN网络的目标训练。

由RPN得到Region Proposal在根据概率值筛选后经过类似的标记过程，被传入R-CNN子网络，进行多分类和坐标回归，同样用多任务损失将二者的损失联合。

**Faster R-CNN的成功之处在于用RPN网络完成了检测任务的"深度化"。使用滑动窗口生成anchor box的思想也在后来的工作中越来越多地被采用（YOLO v2等）。这项工作奠定了"RPN+RCNN"的两阶段方法元结构，影响了大部分后续工作。**

<img width="60%" src="https://i.loli.net/2020/04/06/wWrFczhEGp4J5d3.jpg" alt="Faster RCNN" />

<img width="60%" src="https://i.loli.net/2020/04/07/lQxKi7ONImoEUXH.jpg" alt="Faster RCNN网络架构" />

Faster RCNN针对Fast RCNN做了改进，重点是：
1. 加入了专门生成候选区的神经网络RPN，代替Fast RCNN中Selective Search方法

<img width="60%" src="https://i.loli.net/2020/04/07/DAwVKJrcMFQtWsT.jpg" alt="RPN" />

2. RPN中引入了anchor机制（每个点生成9个anchor）

<img width="60%" src="https://i.loli.net/2020/04/07/9BoNEROQKkZruP8.jpg" alt="anchor" />

<img width="60%" src="https://i.loli.net/2020/04/07/R7SAvjx18gwYuWn.jpg" alt="Faster RCNN训练流程" />

#### Mask R-CNN

<img width="60%" src="https://i.loli.net/2020/03/25/BN9DnWY3qeQfoE8.jpg" alt="Mask R-CNN" />

<img width="60%" src="https://i.loli.net/2020/03/25/y7HT1z4R3KbWDQ5.jpg" alt="R-CNN系列" />

### one-stage
单阶段模型没有中间的区域检出过程，直接从图片获得预测结果，也被成为Region-free方法。

one-stage系列目标检测算法的特点：
- 使用CNN卷积特征
- 直接回归物体的类别概率和位置坐标值（无region proposal）
- 准确度低，速度相对two-stage快

#### YOLO

<img width="80%" src="https://i.loli.net/2020/03/25/NH9VdW2x7CAmj4o.jpg" alt="YOLO" />

YOLO的工作流程如下：
1. 准备数据：将图片缩放，划分为等分的网格，每个网格按跟Ground Truth的IoU分配到所要预测的样本。
2. 卷积网络：由GoogLeNet更改而来，每个网格对每个类别预测一个条件概率值，并在网格基础上生成B个box，每个box预测五个回归值，四个表征位置，第五个表征这个box含有物体（注意不是某一类物体）的概率和位置的准确程度（由IoU表示）。
3. 后处理：使用NMS（Non-Maximum Suppression，非极大抑制）过滤得到最后的预测框

损失函数被分为三部分：坐标误差、物体误差、类别误差。为了平衡类别不均衡和大小物体等带来的影响，损失函数中添加了权重并将长宽取根号。

**YOLO提出了单阶段的新思路，相比两阶段方法，其速度优势明显，实时的特性令人印象深刻。但YOLO本身也存在一些问题，如划分网格较为粗糙，每个网格生成的box个数等限制了对小尺度物体和相近物体的检测。**

#### YOLO V1
<img width="80%" src="https://i.loli.net/2020/03/25/1Aqjgb4Tz7C6uKv.jpg" alt="YOLOV1-1" />

<img width="80%" src="https://i.loli.net/2020/03/25/cZnHb7BE3NYK4Lv.jpg" alt="YOLOV1-2" />

<img width="60%" src="https://i.loli.net/2020/04/06/mi3dH8eTSlfMhAV.jpg" alt="YOLO V1" />

<img width="60%" src="https://i.loli.net/2020/04/06/hTjtAGUbQo6Llzx.jpg" alt="YOLO V1结构" />

<img width="60%" src="https://i.loli.net/2020/04/06/QjX5gYpS6lZN3FI.jpg" alt="YOLO V1流程" />

<img width="60%" src="https://i.loli.net/2020/04/06/QmCOWNYDqR46Fdh.jpg" alt="YOLO V1流程2" />

<img width="60%" src="https://i.loli.net/2020/04/06/VYPAXvQ7w61N3kM.jpg" alt="YOLO V1代价函数" />

#### YOLO V2
<img width="80%" src="https://i.loli.net/2020/03/25/MVH9kyZnFUi5du4.jpg" alt="YOLOV2-1" />

<img width="80%" src="https://i.loli.net/2020/03/25/qDsEyIjlnTm5Kbo.jpg" alt="YOLOV2-2" />

<img width="80%" src="https://i.loli.net/2020/03/25/QLJN6GX5ogqHMdD.jpg" alt="YOLOV2-3" />

<img width="60%" src="https://i.loli.net/2020/04/06/7wQRpLFd4DSXIBi.jpg" alt="YOLO V2" />

<img width="60%" src="https://i.loli.net/2020/04/06/G6sU8hSCXtTf5Hi.jpg" alt="YOLO V2-2" />

<img width="60%" src="https://i.loli.net/2020/04/06/m9DtxjwZgLsXJUq.jpg" alt="YOLO V2-3" />

<img width="60%" src="https://i.loli.net/2020/04/07/juO7Aahd5F4oxKH.jpg" alt="YOLO V2-4" />

<img width="60%" src="https://i.loli.net/2020/04/07/2TWdmixDeo4sSa5.jpg" alt="YOLO V2-4-2" />

<img width="60%" src="https://i.loli.net/2020/04/07/irOVv81PxykjFWA.jpg" alt="YOLO V2-5" />

<img width="60%" src="https://i.loli.net/2020/04/07/ua5T1ldexNUAEgP.jpg" alt="YOLO V2-6" />

#### YOLO V3
[YOLO v3有哪些新特点？](https://juejin.im/post/5c360346f265da6125784d05)
<img width="80%" src="https://i.loli.net/2020/03/25/XUOWkoJD8GjgAFB.jpg" alt="YOLOV3-1" />

<img width="80%" src="https://i.loli.net/2020/03/25/UZa971gs5MQl3Dm.jpg" alt="YOLOV3-2" />

<img width="60%" src="https://i.loli.net/2020/04/07/6FXo5fy8h4dPTcz.jpg" alt="YOLO V3" />

#### SSD
SSD是Single Shot Multi-box Detector的简称，这类将区域提名以及位置和分类合并到一起的方法被称为single shot，上图是Single Shot的另一种方式multi-box detector的结构图。multi-box detector也用到了VGG16，不过仅有前三个conv_block，剔除了全链接层，原先的FC6、FC7又添加了新的卷积。可以看到其中有若干个卷积块连接到了最后的detections，也就是在不同尺度的特征映射上都进行一次物体猜测，这样精度会稍有提高，对于尺度变化较大的物体也能起到较好的效果。

<img width="80%" src="https://i.loli.net/2020/03/25/vHoElGfKwnWNkST.jpg" alt="SSD" />

SSD相比YOLO有以下突出的特点：
- 多尺度的feature map：基于VGG的不同卷积段，输出feature map到回归器中。这一点试图提升小物体的检测精度。
- 更多的anchor box，每个网格点生成不同大小和长宽比例的box，并将类别预测概率基于box预测（YOLO是在网格上），得到的输出值个数为(C+4)×k×m×n，其中C为类别数，k为box个数，m×n为feature map的大小。

**SSD是单阶段模型早期的集大成者，达到跟接近两阶段模型精度的同时，拥有比两阶段模型快一个数量级的速度。后续的单阶段模型工作大多基于SSD改进展开。**

<img width="60%" src="https://i.loli.net/2020/04/07/1yzKFlC96QLdTmU.jpg" alt="SSD算法" />

观察上图，会看到每一层都有指向尾层特征的连线，尾层特征会收集到不同尺寸的特征图。一方面，浅层特征和深层特征包含的信息是不一样的，另一方面，每个层对应的感受野也不同，可以检测不同大小的目标物体。

**SSD算法流程：**

<img width="60%" src="https://i.loli.net/2020/04/07/ElnaqCUMHZcguVP.jpg" alt="SSD流程" />

SSD中使用到了hard negative mining和data augmentation优化手段：
1. hard negative mining

<img width="60%" src="https://i.loli.net/2020/04/07/YnPgFjbCGukophx.jpg" alt="hard" />

2. data augmentation

<img width="60%" src="https://i.loli.net/2020/04/06/xXFBepOmrKWZbvH.jpg" alt="DA" />

**SSD结果分析：**
1. 更多的feature map可以得到更好的结果
2. 使用图片边界的标注框比不使用图片边界的标注框效果更好

**SSD缺点：**
<img width="60%" src="https://i.loli.net/2020/04/06/TFoOgdan5u1DWkY.jpg" alt="SSD缺点" />

## 目标分割算法
<img width="60%" src="https://i.loli.net/2020/04/07/k5vL2uNncryiTS9.jpg" alt="目标分割" />

目标分割的不同层次：
1. 普通分割——将不同类别物体的像素区域分开
2. 语义分割——在普通分割的基础上，分类出每一块区域的语义（判断是什么物体）
3. 实例分割——在语义分割的基础上，给每一个物体编号

### FCN
<img width="60%" src="https://i.loli.net/2020/04/07/IctN5vsZrOheiVz.jpg" alt="FCN算法" />

<img width="60%" src="https://i.loli.net/2020/04/07/BiLvoNbCFq6JXZP.jpg" alt="FCN算法2" />

### Mask RCNN
<img width="60%" src="https://i.loli.net/2020/04/07/eHZoD4v1fJlQASh.jpg" alt=" Mask RCNN" />

<img width="60%" src="https://i.loli.net/2020/04/07/UTti65ofFDQ7zaK.jpg" alt=" Mask RCNN架构" />