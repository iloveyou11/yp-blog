---
title: Bert模型及其变种
date: 2020-07-16
categories: AI
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

Bert（Bidirectional Encoder Representations from Transformers）说白了就是`transformer`中的`encoder`部分，就是怎么将transformer模型中的特征学习出来，是Word2Vec的替代者。

1. 使用了Transformer作为算法的主要框架，能更彻底的捕捉语句中的双向关系；
2. 使用了Mask Language Model(MLM) 和 Next Sentence Prediction(NSP) 的多任务训练目标；
3. 使用更强大的机器训练更大规模的数据，使BERT的结果达到了全新的高度，并且Google开源了BERT模型，用户可以直接使用BERT作为Word2Vec的转换矩阵并高效的将其应用到自己的任务中。

#### 什么是预训练模型
假设我们有大量的维基百科数据，那么我们可以用这部分巨大的数据来训练一个泛化能力很强的模型，当我们需要在特定场景使用时，例如做文本相似度计算，那么，只需要简单的修改一些输出层，再用我们自己的数据进行一个增量训练，对权重进行一个轻微的调整。预训练的好处在于在特定场景使用时不需要用大量的语料来进行训练，节约时间效率高效，bert就是这样的一个泛化能力较强的预训练模型。

BERT预训练模型的输出结果，无非就是一个或多个向量。下游任务可以通过精调（改变预训练模型参数）或者特征抽取（不改变预训练模型参数，只是把预训练模型的输出作为特征输入到下游任务）两种方式进行使用。

#### 如何训练Bert
BERT是一个多任务模型，它的任务是由两个自监督任务组成，即MLM和NSP。

**任务1：Masked Language Model（MLM）**
在训练的时候随机从输入预料上mask掉一些单词（有15%的词汇被随机mask掉），然后通过的上下文预测该单词。交给模型自己去预测被mask的是什么，类似于完形填空。

**任务2：Next Sentence Prediction（NSP）**
预测两个句子是否应该连在一起，句子B是否是句子A的下文。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。

<img src="https://i.loli.net/2020/07/13/4HJVnyOFmdNA9Eh.png" width="80%" alt="bert-句子判断" />

#### 举例说明
例如阅读理解题，输入文章和问题看，输出答案的位置。

如何设计网络呢？需要分别计算答案的起始位置和终止位置，如下图所示：

<img src="https://i.loli.net/2020/07/13/9wmjBbWXN5pTseY.png" width="80%" alt="bert-阅读" />

#### Bert模型如何使用

打开github/bert，下载pre-trained models，在此基础上做fine-tuning操作。

建议练手github/bert上列举的项目。
0. 中文项目`git clone https://github.com/ProHiryu/bert-chinese-ner`
1. download github/bert的代码，放入新建的项目文件夹（`git clone https://github.com/google-research/bert`）
2. 根据不同的任务，选择下载预训练模型，放入checkpoint文件夹
3. 下载数据集
4. 开始训练
5. 在output文件下面，可以找到eval_results.txt文件，就是训练的结果

#### Bert模型的变种

#### 对于Bert的思考

> BERT适用场景
**第一，如果NLP任务偏向在语言本身中就包含答案，而不特别依赖文本外的其它特征，往往应用Bert能够极大提升应用效果。**典型的任务比如QA和阅读理解，正确答案更偏向对语言的理解程度，理解能力越强，解决得越好，不太依赖语言之外的一些判断因素，所以效果提升就特别明显。反过来说，对于某些任务，除了文本类特征外，其它特征也很关键，比如搜索的用户行为／链接分析／内容质量等也非常重要，所以Bert的优势可能就不太容易发挥出来。再比如，推荐系统也是类似的道理，Bert可能只能对于文本内容编码有帮助，其它的用户行为类特征，不太容易融入Bert中。
**第二，Bert特别适合解决句子或者段落的匹配类任务。**就是说，Bert特别适合用来解决判断句子关系类问题，这是相对单文本分类任务和序列标注等其它典型NLP任务来说的，很多实验结果表明了这一点。而其中的原因，我觉得很可能主要有两个，一个原因是：很可能是因为Bert在预训练阶段增加了Next Sentence Prediction任务，所以能够在预训练阶段学会一些句间关系的知识，而如果下游任务正好涉及到句间关系判断，就特别吻合Bert本身的长处，于是效果就特别明显。第二个可能的原因是：因为Self Attention机制自带句子A中单词和句子B中任意单词的Attention效果，而这种细粒度的匹配对于句子匹配类的任务尤其重要，所以Transformer的本质特性也决定了它特别适合解决这类任务。
**第三，Bert的适用场景，与NLP任务对深层语义特征的需求程度有关。**感觉越是需要深层语义特征的任务，越适合利用Bert来解决；而对有些NLP任务来说，浅层的特征即可解决问题，典型的浅层特征性任务比如分词，POS词性标注，NER，文本分类等任务，这种类型的任务，只需要较短的上下文，以及浅层的非语义的特征，貌似就可以较好地解决问题，所以Bert能够发挥作用的余地就不太大，有点杀鸡用牛刀，有力使不出来的感觉。
这很可能是因为Transformer层深比较深，所以可以逐层捕获不同层级不同深度的特征。于是，对于需要语义特征的问题和任务，Bert这种深度捕获各种特征的能力越容易发挥出来，而浅层的任务，比如分词／文本分类这种任务，也许传统方法就能解决得比较好，因为任务特性决定了，要解决好它，不太需要深层特征。
**第四，Bert比较适合解决输入长度不太长的NLP任务，而输入比较长的任务，典型的比如文档级别的任务，Bert解决起来可能就不太好。**主要原因在于：Transformer的self attention机制因为要对任意两个单词做attention计算，所以时间复杂度是n平方，n是输入的长度。如果输入长度比较长，Transformer的训练和推理速度掉得比较厉害，于是，这点约束了Bert的输入长度不能太长。所以对于输入长一些的文档级别的任务，Bert就不容易解决好。结论是：Bert更适合解决句子级别或者段落级别的NLP任务。          出自[《一文读懂BERT(原理篇)》](https://blog.csdn.net/jiaowoshouzi/java/article/details/89073944)

#### Bert源码解析