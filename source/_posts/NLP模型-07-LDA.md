---
title: NLP模型-07-LDA
date: 2020-07-17
categories: NLP
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

[NLP模型-01-预训练语言模型](https://iloveyou11.github.io/2020/07/14/NLP%E6%A8%A1%E5%9E%8B-01-%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/)
[NLP模型-02-HMM](https://iloveyou11.github.io/2020/07/14/NLP%E6%A8%A1%E5%9E%8B-02-HMM/)
[NLP模型-03-GMM](https://iloveyou11.github.io/2020/07/15/NLP%E6%A8%A1%E5%9E%8B-03-GMM/)
[NLP模型-04-CRF](https://iloveyou11.github.io/2020/07/15/NLP%E6%A8%A1%E5%9E%8B-04-CRF/)
[NLP模型-05-transformer](https://iloveyou11.github.io/2020/07/16/NLP%E6%A8%A1%E5%9E%8B-05-transformer/)
[NLP模型-06-Bert](https://iloveyou11.github.io/2020/07/16/NLP%E6%A8%A1%E5%9E%8B-06-Bert/)
[NLP模型-07-LDA](https://iloveyou11.github.io/2020/07/17/NLP%E6%A8%A1%E5%9E%8B-07-LDA/)
[NLP模型-08-fastText](https://iloveyou11.github.io/2020/07/17/NLP%E6%A8%A1%E5%9E%8B-08-fastText/)
[NLP模型-09-Glove](https://iloveyou11.github.io/2020/07/17/NLP%E6%A8%A1%E5%9E%8B-09-Glove/)
[NLP模型-10-textRNN & textCNN](https://iloveyou11.github.io/2020/07/18/NLP%E6%A8%A1%E5%9E%8B-10-textRNN%20&%20textCNN/)
[NLP模型-11-seq2seq](https://iloveyou11.github.io/2020/07/18/NLP%E6%A8%A1%E5%9E%8B-11-seq2seq/)
[NLP模型-12-attention](https://iloveyou11.github.io/2020/07/18/NLP%E6%A8%A1%E5%9E%8B-12-attention/)
[NLP模型-13-XLnet](https://iloveyou11.github.io/2020/07/19/NLP%E6%A8%A1%E5%9E%8B-13-XLnet/)
[NLP模型-14-核方法](https://iloveyou11.github.io/2020/07/22/NLP%E6%A8%A1%E5%9E%8B-14-%E6%A0%B8%E6%96%B9%E6%B3%95/)

### 什么是LDA？
LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。

**LDA的目的就是要识别主题，即把文档—词汇矩阵变成文档—主题矩阵（分布）和主题—词汇矩阵（分布）**

### 生成过程
LDA 的产生过程描述了文档以及文档中文字的生成过程。在原始的 LDA 论文中，作者们描述了对于每一个文档而言有这么一种生成过程：

<img src="https://i.loli.net/2020/07/12/UDbg9mChKXN61YL.png" alt="LDA生成流程" width="60%" />

1. 从一个全局的泊松（Poisson）参数为β的分布中生成一个文档的长度 N；
2. 从一个全局的狄利克雷（Dirichlet）参数为α的分布中生成一个当前文档的θ；
3. 然后对于当前文档长度 N 的每一个字执行以下两步：
  - 一是从以θ为参数的多项（Multinomial）分布中生成一个主题（Topic）的下标（Index）z_n
  - 二是从以φ和 z 共同为参数的多项分布中产生一个字（Word）w_n。

**α->θ的生成、β->Fai的生成：**
需要满足条件
1）和为1 ；
2）大于0，采用Dirichlet分布生成

**θi->Zij的生成（在主题分布中进行主题采样）：**
使用multinomial分布进行采样


### PLSA与LDA区别
PLSA确定了主题概率，LDA未确定固定的主题概率，只规定了主题概率服从Dirichlet分布
**PLSA**
1. 按照概率`P(di)`选择一篇文档`di`
2. 选定文档`di`后，确定文章的主题分布
3. 从主题分布中按照概率`P(zk/di)`选择一个隐含的主题类别`zk`
4. 选定zk后，确定主题下的词分布
5. 从词分布中按照概率`P(wj/zk)`选择一个词`wij `

<img src="https://i.loli.net/2020/07/12/GHMkD4ofsBpxw7J.png" alt="PLSA与LDA" width="60%" />

### LSA与LDA区别
LSA是latent semantic analysis，是一种信息检索模型，具体的流程如下：
1. 分析文档集合，建立词汇-文档矩阵（各词汇在各文档中的出现情况，矩阵非常稀疏）
2. 对词汇-文档矩阵进性奇异值分解（提取稀疏矩阵的特征）
3. 对SVD分解后的矩阵进行降维（提取这些特征中比较重要的特征）
4. 使用降维后的矩阵构建潜在语义空间（用比较重要的特征来表征词或文本）

**LDA**
1. 按照先验概率`P(di)`选择一篇文档`di`
2. 从Dirichlet分布`α`中采样生成文档`di`的主题分布`θi`（`θi`由超参数`α`的Dirichlet分布生成）
3. 从主题分布`θi`中采j样生成文档`di`第j个词的主题`zij`
4. 从Dirichlet分布`β`中采样生成主题`zij`对应的词分布`FAIij`（`FAIij`由超参数`β`的Dirichlet分布生成）
5. 从词分布中按照概率`FAIij`选择一个词`wij `

### LDA模型变种
扩展 LDA 的思路：
1. **上游扩展法**：依靠额外的信息去影响主题分布，进而影响文档字句的选择。
2. **下游扩展法**：把其他信息都是放在主题变量的下游的，希望通过主题变量来施加影响。

#### 作者LDA
“作者LDA”又称作“用于作者和文档信息的作者主题模型”。
每篇文档都会有一些作者信息，我们可以把这些作者编码成为一组下标（Index）。对于每一个文档来说，我们首先从这组作者数组中，选出一个当前的作者，然后假定这个作者有一组相对应的主题。这样，文档的主题就不是每个文档随机产生了，而是每个作者有一套主题。这个时候，我们从作者相对应的主题分布中取出当前的主题，然后再到相应的语言模型中，采样到当前的单词。**主题分布不是每个文档有一个，而是每个作者有一个。**

#### 时间动态LDA
把文档放到时间的尺度上，希望去分析和了解文档在时间轴上的变化。
状态空间模型就是把不同时间点的狄利克雷分布的参数给串起来，使得这些分布的参数会随着时间的变化而变化，把一堆静态的参数用状态空间模型串接起来。