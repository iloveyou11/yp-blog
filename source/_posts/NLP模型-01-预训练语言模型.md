---
title: NLP模型-01-预训练语言模型
date: 2020-03-20
categories: NLP
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

### 什么是预训练模型
预训练模型实际上就是已经训练过的模型或模型组件。它已经在大型训练集上经过了很好的训练，学习到了每个词的表示，并且保存下来所有的模型参数。

预训练模型的本质是用来文本特征提取，可以适用于迁移学习，减少前期训练词向量的成本。

预训练模型在word embedding就开始出现，相当于AlexNet在卷积神经网络的地位（雏形）。word embedding是将文字进行向量化，探索字词句的向量表示，如one-hot、tfidf等表示方法，但是这种表达方式非常稀疏，而且词的含义并没有得到表达，因此我们要探索稠密向量。

在自然语言处理领域的背景下，预训练技术通过使用大规模无标注的文本语料来训练深层网络结构，从而得到一组模型参数，这种深层网络结构通常被称为“预训练模型”;将预训练好的模型参数应用到后续的其他特定任务上，这些特定任务通常被称为“下游任务”。

预训练技术取得的巨大成功，很大程度上归功于其实现了`迁移学习`的概念。迁移学习本质上是在一个数据集上训练基础模型，通过微调等方式，使得模型可以在其他不同的数据集上处理不同的任务。预训练的过程如上文所述，是将预训练好的模型的相应结构和权重直接应用到下游任务上，从而实现“迁移学习”的概念，即将预训练模型“迁移”到下游任务。

词向量的表达应该符合以下特征：
1. 在分布空间中，含义越类似的词语应该距离越近（相同类型的词语应该在同一簇）
2. 不同语言，相同含义的词语应该出现在分布空间的同一位置
3. 词语应该支持加减法，如`king-queue=man-woman`

**预训练模型应该如何使用呢？**
1. 直接将预训练模型当做特征提取来使用
2. 只采用预训练模型的结构，所有权重随机初始化，使用自己的数据集进行重新训练
3. Fine-tune，冻结一部分结构，重新训练剩下的层，或者加上一些层

**预训练模型分为以下三类：**
1. 自回归语言模型，如GPT
2. 去噪自编码DAE，如Bert
3. 排列语言模型，如XLNet

### 分布式表示
以下是词向量表示法的分类：

<img src="https://i.loli.net/2020/07/20/pJaOoIxtEYBemMW.png" alt="分布式表示" width="100%" />

从上图可以看出，词语的分布式表示分为`欧式空间`和`非欧式空间`，其中`非欧式空间`常常是高维的表达，如黎曼空间、球形空间等等，距离计算也均不相同，我们常见的是欧式空间的表达。

欧式空间可以分为`global`方法和`local`方法，`global`方法会考虑整个句子词与词之间的相关性，从而衡量词语的向量表达，如MF（矩阵分解）、LDA（主题模型）。这些方法需要考虑到整个语料库的信息，但是有个缺点，就是在训练好词向量后，如果加入了新词或者改变了上下文，整个词向量矩阵均需要重新计算，这个计算量还是不容小觑的。因此，基于这个不足，后面衍生出了`local`的词向量表达——只考虑了一篇文档、一句话或者是上下文的几个词语，而不是完整的语料库，这样能加快更新的速度。

`local`方法分为`基于LM的训练`和`基于非LM的训练`，其本质区分点在于是否使用了语言模型。这里通俗地解释一下语言模型——就是衡量一句话是否是人话的程度。
`基于非LM的训练`包括了Skip-Gram、CBOW（连续词袋模型）、Bert、ALBert等等，CBOW是使用上下文的词语预测中心词语，Skip-Gram是使用中心词语预测上下文的词语。两者在训练的过程中，运用了一些小技巧，如层次softmax、Negative Sampling等等，后文会有详细解释。CBOW和Skip-Gram依然存在一些问题，例如碰到一词多义的情景就不知道该怎么办了，总不能用一个固定的词向量表示吧。

于是就有了`local>基于非LM的训练>考虑上下文`的模型，如Bert、ALBert等等。transformer是在LSTM之后效果最佳的特征提取器，模型中使用到了attention机制。Bert就是transformer中的Encoder。

`local>基于LM的训练`中考虑上下文的模型有EMLo、XLNet，不考虑上下文的模型有NNLM。

#### EMLo
EMLo能够很好地解决一词多义的问题，它的basic unit是LSTM，底层使用已经训练好的静态词向量（如Glove），再接上两层双向LSTM双向网络，通过多层进行特征抽取（如单词特征、句法特征、语义特征），最后每个词语都会得到3个向量，分别为底层、第一层LSTM、第二层LSTM，使用在任务中学习到的权重，对这三项进行加权平均，从而得到最终的词向量。

EMLo虽然指标做到了当时的SOTA，但是仍然存在一些问题：
1. EMLo的LSTM抽取特征的能力是远低于如今的transformer的
2. 使用拼接方式实现双向网络，特征融合的能力还是较弱的，其本质上还是利用了从左到右、从右到左的两个网络

#### GPT
GPT（Improving Language Understanding by Generative Pre-Training）就是Transformer的decoder，采用了masked self-attention的训练方式。它的核心思想，是利用`一个网络模型`去做`预训练任务`和`下游任务`，将下游任务的输入全部处理成满足预训练模型的输入格式，这样可以直接使用预训练模型去做任务了。我们只需要在一句话的前后添加两个token标志即可。

不同于EMLo的LSTM结构，GPT使用transformer能够捕捉到更长范围内的语义信息，且transformer的效果要优于LSTM。

GPT2相对于GPT来说，并没有太大实质性的改动，增加了更高质量、更广泛、数量更大的数据集，采用了更巨大的transformer结构，且对transformer结构做了一些微调。GPT2的目标，是**使用一个大量和质量好的无标签数据，去完成NLP下游任务（使用无监督模型去做监督任务）**。

GPT2是一种生成模型，拥有了15亿的参数，在阅读理解任务上能够获得惊人的效果，但是在文本摘要生成上并没有取得理想的效果。

#### BERT
Bert是transformer结构的encoder，具体的详细解释见[《Bert模型及其变种》](https://iloveyou11.github.io/2020/07/16/Bert%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E5%8F%98%E7%A7%8D/)

目前，Bert模型在诸多NLP任务中都取得了不错的效果，如：
- 分类：文本分类、情感分类……
- 序列标注：分词、实体识别、语义标注……
- 句子关系：问答、推理……
- 生成任务：文本摘要、机器翻译……

Bert仍然存在一些缺点：
- 训练和测试的数据格式不一致（测试阶段没有mask token）
- 词语之间保持了独立
- 无法处理长文本

#### RoBERTa
RoBERTa（a Robustly Optimized BERT Pretraining Approach）是Bert的一个强劲改进版本，它使用了更大的数据、更大的batch size、更长的训练时间、更大的学习率（超参数调整），并且将静态的mask改为了动态的mask，获得了不错的效果（超越了之前出现的模型，当时的SOTA）。

#### ALBERT
在RoBERTa出现后的几天，ALBERT（A Lite BERT For Self-Supervised Learning Of Language Representations）就超越了RoBERTa成为当时最先进的算法。ALBERT有以下这些创新点：
- 词嵌入向量参数的因式分解
- 跨层参数共享 
- NSP 预训练任务
- 去掉dropout、LAMB优化器、更大的batch-size
- N-gram mask