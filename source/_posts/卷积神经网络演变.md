---
title: 卷积神经网络的演变
date: 2020-03-16
categories: AI
author: yangpei
comments: true
cover_picture: /images/banner.jpg
---

本文主要介绍深度学习种卷积神经网络的发展历史，以及典型的卷积神经网络。

<!-- more -->



### 基础介绍
#### 什么是深度学习
深度学习是指一类对具有深层结构的神经网络进行有效训练的方法。神经网络是一种由许多非线性计算单元（神经元）组成的分层系统，网络深度是不包括输入层的层数。

#### 什么是卷积神经网
卷积神经网络是一种特殊的多层感知机或前馈神经网络，具有局部连接、权值共享的特点，其中大量神经元按照一定方式组织起来对视野中的交叠区域产生反应。

卷积神经网络是深度学习中最为重要的模型。在2012年，提出了著名的AlexNet，在ImageNet（大规模图片分类竞赛）上取得了优异成绩，为深度学习奠定了基础。之后，卷积神经网络相继出现，如VGGNet、GoogLeNet、ResNet、SPPNet、DenseNet、Faster RCNN、YOLO、SSD、FCN、Mask RCNN、DCGAN等等，极大地推进了图片分类、识别和理解技术的发展。

#### 深度学习模型
- 卷积神经网络（CNN）
- 循环神经网络（RNN）
- 长短期记忆网络（LSTM）
- 深层自编码器（AE）
- 深层信念网络
- 强化学习网络（RLN）
- 生成式对抗网络（GAN）
- 受限玻尔兹曼机（RBM）
- 深层玻尔兹曼机（DSN）
……

### 相关知识
#### 激活函数
常见激活函数有sigmoid、tanh、relu、leaky relu、maxout等等，它们的函数图像如下：
<img width="50%" alt="激活函数" src="https://i.loli.net/2020/03/23/PDkzl9BehuEnZ54.png"/>

**特点及问题：**
**1. Sigmoid**
- Sigmoid函数饱和使梯度消失。当神经元的激活在接近0或1处时会饱和，在这些区域梯度几乎为0，这就会导致梯度消失，几乎就有没有信号通过神经传回上一层。
- Sigmoid函数的输出不是零中心的。因为如果输入神经元的数据总是正数，那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降。

**2. tanh**
- Tanh解决了Sigmoid的输出是不是零中心的问题，但仍然存在饱和问题。
- 为了防止饱和，现在主流的做法会在激活函数前多做一步batch normalization，尽可能保证每一层网络的输入具有均值较小的、零中心的分布。

**3. ReLU**
- ReLU单元比较脆弱并且可能“死掉”，而且是不可逆的，因此导致了数据多样化的丢失。通过合理设置学习率，会降低神经元“死掉”的概率。

**4. Leaky ReLU**
- 能解决了ReLU神经元“死掉”的问题

**5. Maxout**
- Maxout是对ReLU和leaky ReLU的一般化归纳。Maxout具有ReLU的优点，如计算简单，不会 saturation，同时又没有ReLU的一些缺点，如容易go die。问题就是每个神经元的参数double（翻倍），这就导致整体参数的数量激增。

**6. Softmax**
- Softmax用于多分类神经网络输出，目的是让大的更大。

#### 优化器（梯度下降算法）
- **随机梯度下降（SGD）**
分为两种模式：整体梯度下降、mini梯度下降。**整体梯度下降**是先把所有样本随机洗牌，再逐一计算每个样本对梯度的贡献去更新权值。缺点是梯度下降的过程不太稳定、波动较大。**mini梯度下降**是将所有样本随机洗牌后分为若干大小为m的块，再逐一计算每块对梯度的贡献去更新权值。
- **Momentum动量**
加一个动量（借助物理学的惯性原理）

<img width="50%" src="https://i.loli.net/2020/03/23/EP24GoBwSs8jNlx.jpg" alt="Momentum动量"/>

- **Adagrad**
在不同方向上的学习率是可以变化的，计算时除了这个方向上所有之前梯度的平均值。

<img width="50%" src="https://i.loli.net/2020/03/23/A5ougkDGOqzniVy.jpg" alt="Adagrad">

- **RMSProp**
在不同方向上的学习率是可以变化的，计算时除了这个方向上所有之前梯度的加权平均值（最近的gradient权重大，过去的gradient权重小）

<img width="50%" src="https://i.loli.net/2020/03/23/EB9vHFjotcremDW.jpg" alt="RMSProp">

- **Adam**
记录过去一段时间的梯度平方和（类似Adagrad和RMSProp）以及梯度的和（类似Momentum动量），把优化看作是铁球滚下山坡，定义了一个带动量和摩擦的铁球。Adam是目前最好的算法，在不知道如何选择时就选择它。

- ……

#### 权值(w)偏置(b)初始化
在训练神经网络之前，必须对其权值和偏置进行初始化，常用的初始化方法有3种：高斯初始化、Xavier初始化和MSRA初始化。它们一般都是把偏置初始化为0，对权值进行随机初始化。

#### 过拟合和欠拟合
**1. 如何防止过拟合**
- 获取和使用更多的数据（数据集增强）——解决过拟合的根本性方法
- 采用合适的模型（控制模型的复杂度）
- 降低特征的数量
- 使用正则化项（常见的L0、L1、L2正则化项，L1可以稀疏神经元，L2可以降低神经元权重，从而降低了过拟合的风险）
- dropout（在网络训练过程中让网络某些节点不工作，不过度依赖某些神经元）
- early stopping（如果发现随着训练迭代次数的增多，测试集并没有出现损失减少反而上升的现象，则可能是发生了过拟合，导致了泛化能力降低，应提前停止训练）

**2. 如何防止欠拟合**
- 增加网络复杂度
- 在模型中增加特征

### 常见问题
#### 为什么需要激活函数？
如果不适用激活函数，那么就算层数再深，表达的也只是线性函数，和使用一层网络表达的效果相同。

#### 网络加深就一定好吗？
加深在一定程度上可以提升模型性能，但是未必就是网络越深越越好。深层网络带来的梯度不稳定，网络退化的问题始终都是存在的，可以缓解，没法消除。这就有可能出现网络加深，性能反而开始下降。深层网络的训练很少能突破30层，VGGNet19层，GoogleNet22层，MobileNet28层，经典的网络超过30层的也就是ResNet系列常见的ResNet50，ResNet152了。ResNet网络使用跳连结构，有效地解决了梯度消失问题。

模型加深还可能出现的一些问题是导致某些浅层的学习能力下降，限制了深层网络的学习，这也是跳层连接等结构能够发挥作用的很重要的因素。

### 卷积神经网络演变
<img width="60%" alt="卷积神经网络演变历史" src="https://i.loli.net/2020/03/23/tjsBuCpIH5xzlea.jpg"/>

**图像识别系列**

#### LeNet——雏形网络
第一个真正的卷积神经网络在1998年提出，称为LeNet。模型共有8层（不计输入层），包括3个卷积层、2个下采样层、1个全连接层、1个输出层。模型结构如下：

<img width="60%" alt="LeNet" src="https://i.loli.net/2020/03/23/Qf9OoUcmvXJWTxj.jpg"/>

<img width="60%" alt="LeNet2" src="https://i.loli.net/2020/03/23/wbM3USkNtuOjHqm.jpg"/>

#### AlexLet
AlexLet包含5个卷积层（进行了3次最大池化）、3个全连接层。模型结构如下：

<img width="60%" src="https://i.loli.net/2020/03/23/hKbTYGAp715dlNJ.jpg" alt="AlexLet"/>

AlexLet与LeNet相比，有了很多的改进：

1. 使用了ReLU激活函数，提高训练速度（ReLU是一种非饱和函数，在训练时间上比饱和函数sigmoid、tanh快，而且ReLU利用了分片线性结构实现了非线性的表达能力，梯度消失现象较弱，有助于训练更深的网络）
2. 使用GPU训练（可以提供数十倍乃至于上百倍于CPU的性能）
3. 使用重叠池化（传统池化窗口没有重叠，不同窗口池化过程分别独立计算，有助于缓解过拟合）
4. 局部响应归一化（不过后来验证没效果，采用的是BN？）
5. 数据扩充——图像平移和反转、丢失输出——随即丢弃节点（减少了过拟合）

#### SPPNet——空间金字塔
空间金字塔池化网络，在最后一个卷积层和第一个全连接层之间插入了一个空间金字塔池化层，用来池化特征并产生固定长度的输出。无需对输入图像进行裁剪和变形，就可以处理输入图像大小不同的情况。

<img width="60%" alt="SPPNet" src="https://i.loli.net/2020/03/23/eSWywOImAvVGDMn.jpg"/>

SPP有几个引人注目的特征：

1. SPP对于任意输入大小都能产生一个固定长度的输出，而滑动窗口池化不能
2. SPP使用多级大小空间窗口，而滑动窗口池化只使用一个窗口大小
3. SPP可以在不同尺度上提取特征并进行池化

<img width="60%" alt="SPPNet2" src="https://i.loli.net/2020/03/23/mqFH2SLMc1J7XZg.jpg"/>

#### VGGNet
有两种基本类型：VGGNet-16、VGGNet-19。VGGNet全部使用3X3的卷积核和2X2的池化核。VGG 块的组成规律是：连续使用若干个相同的填充为 1 、窗口形状为3X3的卷积层后接上一个步幅为 2 、窗口形状为2X2的最大池化层。

对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。

常见的VGG网络有：VGG-11、VGG-13、VGG-16、VGG-19

<img width="50%" alt="VGGNet" src="https://i.loli.net/2020/03/23/GXiz8tcIZmHfRaM.jpg"/>

#### GoogLeNet
GoogLeNet专注于如何建立更深的网络结构，通视引入新型的基本结构——Inception模块，以加深网络宽度。GoogLeNet包括V1、V2、V3、V4版本。

**Inception的作用：** 代替人工确定卷积层中的过滤器类型或者确定是否需要创建卷积层和池化层，即：不需要人为的决定使用哪个过滤器，是否需要池化层等，由网络自行决定这些参数，可以给网络添加所有可能值，将输出连接起来，网络自己学习它需要什么样的参数。

**1. Inception V1结构**

<img width="50%" alt="Inception v1" src="https://i.loli.net/2020/03/23/YzK3WBeIwZROaVy.jpg"/>

由上图可以看出，Inception 块里有4条并行的线路，它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用1X1卷积层减少通道数从而降低模型复杂度。

**2. Inception V2结构**

<img width="40%" alt="Inception v2" src="https://i.loli.net/2020/03/23/8MEfPrFHmaWiUO9.jpg"/>

用2个连续的3x3卷积层(stride=1)组成的小网络来代替单个的5x5卷积层，这便是Inception V2结构，保持感受野范围的同时又减少了参数量

**3. Inception V3结构**

考虑了 nx1 卷积核，如下图所示的取代3x3卷积：于是，任意nxn的卷积都可以通过1xn卷积后接nx1卷积来替代。

<img width="60%" alt="Inception v3" src="https://i.loli.net/2020/03/23/Vg3MxciEzeQCDqU.jpg"/>

<img width="60%" alt="Inception v3-2" src="https://i.loli.net/2020/03/23/jpGdJlD7NWFH5o8.jpg"/>

**4. Inception V4结构**
它结合了残差神经网络ResNet。

#### ResNet——残差网络
随着网络结构的加深， 梯度消失或梯度爆炸问题会越来越严重，可能导致神经网络学习和训练变得越来越困难。通过初始化、随机丢弃、归一化等技巧可以得到一定程度的缓和，而ResNet使用了在网络中增加信息传递快速通道的方法，信息可以无障碍地跨越多层直接传递到后面的层。

残差网络引入了跨层连接，构造了残差模块。基于残差模块，深层残差网络可以具有非常深的结构，深度甚至可以达到1000层以上。

<img width="30%" alt="ResNet" src="https://i.loli.net/2020/03/23/tisQ4Tep2mVohjI.jpg"/>

#### DenseNet——密连网络
残差网络在层间加入跨层连接，使得即使成百上千层的网络，也可以得到精准地训练。不过，残差网络一般只采用2~3层的跨层连接形成残差模块，密连卷积网络（DenseNet）通过引入密连模块代替残差模块进一步扩展了残差网络的结构。与残差模块的区别在于，密连模块内部允许任意两个非相邻层之间进行跨层连接。

<img width="50%" alt="DenseNet" src="https://i.loli.net/2020/03/23/mU91kI2sqeA3hga.jpg"/>

<img width="50%" alt="DenseNet2" src="https://i.loli.net/2020/03/23/BKoIu2OG649eLWZ.jpg"/>

#### CatNet——拼接网络
CatNet包含了r个交错的卷积层和池化层，再跟一个全连接层和输出层。其中，全连接层是所有卷积层和池化层通过跨层连接拼接的得到的。

<img width="50%" alt="CatNet" src="https://i.loli.net/2020/03/23/2l54TnfwRahcM67.jpg"/>

---

**目标检测系列**

#### R-CNN——区域卷积网络
R-CNN是一种目标检测模型，目标检测要求在图像中确定多个可能目标的位置。R-CNN采用了华东窗口的策略进行定位，包括3大模块：区域推荐、特征提取、区域分类。
1. 区域推荐
给输入图像生成约2000个类别无关的区域推荐构成候选检测集。R-CNN采用的区域推荐方法是选择性搜索（selective search），其他推荐方法包括目标够成度、类别无关目标推荐、受限参数最小割等。
2. 特征提取
利用卷积网络计算每个推荐的特征，要求先将推荐转变为卷积网络的输入大小，并且做减均值处理。
3. 区域分类
对每个区域进行打分和筛选。R-CNN先采用支持向量机SCM对所提取的特征打分，再根据分支高低，通过贪婪非最大抑制策略进行筛选，保留高分推荐。

<img width="60%" alt="RCNN" src="https://i.loli.net/2020/03/23/1DHYvwhOuKnNcaC.jpg"/>

#### Fast R-CNN
R-CNN利用深层卷积网络对目标区域推荐进行分类，可以得到较高的检测精度，但也有明显不足：

1. 训练过程阶段多。先区域推荐、再求特征、再区域分类
2. 训练时空费用大。支持向量机和边框回归都需要从每幅图像的每个区域推荐提取特征写入硬盘，过程非常耗时。
3. 目标检测速度慢。

于是有人提出Fast R-CNN网络，优点在于1）检测质量更高 2）训练过程统一 3）网络圈层更新 4）无需磁盘存储

Fast R-CNN的结构如下：

<img width="60%" alt="Fast R-CNN0" src="https://i.loli.net/2020/03/23/cI3ALbZ9adwymjU.jpg"/>

输入一幅完整图像和多个感兴趣区域（RoI）,京埚几个卷积和最大池化层的处理，产生一个共享卷积特征图，用来为每个区域推荐的RoI通过最大池化提取一个固定长度的特征向量。这些特征向量的输入到一系列全连接层后，又分化为两个并列输出层：一个输出类别softmax概率，一个输出目标惊喜边框位置。

<img width="60%" alt="Fast R-CNN" src="https://i.loli.net/2020/03/23/cyR4uFf5DiOr6Pj.jpg"/>

<img width="60%" alt="Fast R-CNN2" src="https://i.loli.net/2020/03/23/aYZt7g2bEomSreN.jpg"/>

Fast R-CNN做了以下改变：

1. 把最后的最大池化层替换为一个RoI池化层
2. 把最后的全连接层和softmax层替换为两个兄弟层，分别用来估计每个类别的概率和边框
3. 把网络改为接收两种数据输入，一是图像列表，二是RoooI列表

#### Faster R-CNN
Faster R-CNN采用了区域推荐网络（RPN），可以检测网络共享整幅图像的卷积特征，从而产生几乎无代价的区域推荐。

<img width="30%" alt="Faseter RCNN" src="https://i.loli.net/2020/03/23/TzXD2xKCI3Fpwqv.jpg"/>

Faster R-CNN有两个模块：一是用来产生区域推荐的RPN，二是使用推荐于去的Fast R-CNN检测器。整个系统是一个统一的目标检测网络，其中RPN采用了attention机制，告诉Fast R-CNN模块应该看什么地方。

#### Mask R-CNN

<img width="60%" src="https://i.loli.net/2020/03/23/INtbYW4eXKJcuAr.jpg" alt="Mask R-CNN"/>

**RCNN系列算法对比：**

<img width="60%" src="https://i.loli.net/2020/03/23/FagjoMOD3JS78VU.jpg" alt="RCNN系列算法对比"/>

#### YOLO

<img width="50%" alt="YOLO" src="https://i.loli.net/2020/03/23/5rClbMTnG3XhHq9.jpg"/>

#### SSD
单次检测器（SSD）是一种新型的深度神经网络目标检测器，不用对边框假设重采像素或特征，也不会损失精度，但速度比Faster R-CNN、YOLO都要快。SSD提高速度的根本改进措施是消除边框推荐和随后的像素或特征重采样阶段，还包括使用小卷积核在变量位置预测对象的类别和偏移，使用独立预测器负责不同高度比的检测，并用这些滤波器在网络后期的多个特征图种执行多尺度检测。